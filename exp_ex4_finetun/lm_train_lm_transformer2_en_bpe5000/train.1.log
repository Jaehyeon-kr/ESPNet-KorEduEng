# python3 -m espnet2.bin.lm_train --ngpu 4 --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/org/devFix.example/text,text,text --valid_shape_file exp_ex3/lm_stats_en_bpe5000/valid/text_shape.bpe --fold_length 150 --resume true --output_dir exp_ex3/lm_train_lm_transformer2_en_bpe5000 --config conf/tuning/train_lm_transformer2.yaml --train_data_path_and_name_and_type dump/raw/lm_train.txt,text,text --train_shape_file exp_ex3/lm_stats_en_bpe5000/train/text_shape.bpe --ngpu 4 --multiprocessing_distributed True 
# Started at Fri Jan 24 01:42:31 KST 2025
#
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/bin/python3 /data/bootcamp2501/espnet/espnet2/bin/lm_train.py --ngpu 4 --use_preprocessor true --bpemodel data/en_token_list/bpe_unigram5000/bpe.model --token_type bpe --token_list data/en_token_list/bpe_unigram5000/tokens.txt --non_linguistic_symbols none --cleaner none --g2p none --valid_data_path_and_name_and_type dump/raw/org/devFix.example/text,text,text --valid_shape_file exp_ex3/lm_stats_en_bpe5000/valid/text_shape.bpe --fold_length 150 --resume true --output_dir exp_ex3/lm_train_lm_transformer2_en_bpe5000 --config conf/tuning/train_lm_transformer2.yaml --train_data_path_and_name_and_type dump/raw/lm_train.txt,text,text --train_shape_file exp_ex3/lm_stats_en_bpe5000/train/text_shape.bpe --ngpu 4 --multiprocessing_distributed True
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:42:38,619 (lm:189) INFO: Vocabulary size: 5000
[seoultech:0/4] 2025-01-24 01:42:38,677 (encoder:177) INFO: encoder self-attention layer type = self-attention
[seoultech:0/4] 2025-01-24 01:42:39,443 (abs_task:1271) INFO: pytorch.version=2.1.0, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[seoultech:0/4] 2025-01-24 01:42:39,446 (abs_task:1272) INFO: Model structure:
ESPnetLanguageModel(
  (lm): TransformerLM(
    (embed): Embedding(5000, 128)
    (encoder): Encoder(
      (embed): Sequential(
        (0): Linear(in_features=128, out_features=512, bias=True)
        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): Dropout(p=0.1, inplace=False)
        (3): ReLU()
        (4): Sequential()
      )
      (encoders): MultiSequential(
        (0): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (10): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (11): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (12): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (13): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (14): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (15): EncoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
    )
    (decoder): Linear(in_features=512, out_features=5000, bias=True)
  )
)

Model summary:
    Class Name: ESPnetLanguageModel
    Total Number of model parameters: 53.71 M
    Number of trainable parameters: 53.71 M (100.0%)
    Size: 214.84 MB
    Type: torch.float32
[seoultech:0/4] 2025-01-24 01:42:39,446 (abs_task:1275) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.005
    lr: 2e-07
    maximize: False
    weight_decay: 0
)
[seoultech:0/4] 2025-01-24 01:42:39,446 (abs_task:1276) INFO: Scheduler: WarmupLR(warmup_steps=25000)
[seoultech:0/4] 2025-01-24 01:42:39,447 (abs_task:1285) INFO: Saving the configuration in exp_ex3/lm_train_lm_transformer2_en_bpe5000/config.yaml
[seoultech:0/4] 2025-01-24 01:42:40,746 (abs_task:1663) INFO: [train] dataset:
ESPnetDataset(
  text: {"path": "dump/raw/lm_train.txt", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f1df056a9d0>)
[seoultech:0/4] 2025-01-24 01:42:40,746 (abs_task:1664) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=114, batch_bins=200000000, sort_in_batch=descending, sort_batch=descending)
[seoultech:0/4] 2025-01-24 01:42:40,746 (abs_task:1665) INFO: [train] mini-batch sizes summary: N-batch=114, mean=1197.9, min=65, max=5715
[seoultech:0/4] 2025-01-24 01:42:40,793 (abs_task:1663) INFO: [valid] dataset:
ESPnetDataset(
  text: {"path": "dump/raw/org/devFix.example/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f1df03f0a60>)
[seoultech:0/4] 2025-01-24 01:42:40,793 (abs_task:1664) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=7, batch_bins=200000000, sort_in_batch=descending, sort_batch=descending)
[seoultech:0/4] 2025-01-24 01:42:40,793 (abs_task:1665) INFO: [valid] mini-batch sizes summary: N-batch=7, mean=813.3, min=9, max=2501
[seoultech:0/4] 2025-01-24 01:42:40,826 (abs_task:1663) INFO: [plot_att] dataset:
ESPnetDataset(
  text: {"path": "dump/raw/org/devFix.example/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f1df03f08b0>)
[seoultech:0/4] 2025-01-24 01:42:40,826 (abs_task:1664) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=5693, batch_size=1, key_file=exp_ex3/lm_stats_en_bpe5000/valid/text_shape.bpe, 
[seoultech:0/4] 2025-01-24 01:42:40,826 (abs_task:1665) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
[seoultech:0/4] 2025-01-24 01:42:41,365 (trainer:168) INFO: The training was resumed using exp_ex3/lm_train_lm_transformer2_en_bpe5000/checkpoint.pth
seoultech:1703287:1703287 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703287:1703287 [0] NCCL INFO Bootstrap : Using eno1:117.17.185.208<0>
seoultech:1703287:1703287 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
seoultech:1703287:1703287 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
seoultech:1703287:1703287 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.5+cuda12.1
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:42:42,495 (trainer:299) INFO: 151/200epoch started
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
seoultech:1703289:1703289 [2] NCCL INFO cudaDriverVersion 12020
seoultech:1703289:1703289 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703289:1703289 [2] NCCL INFO Bootstrap : Using eno1:117.17.185.208<0>
seoultech:1703289:1703289 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
seoultech:1703289:1703289 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
seoultech:1703289:1703343 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703289:1703343 [2] NCCL INFO NET/IB : No device found.
seoultech:1703289:1703343 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703289:1703343 [2] NCCL INFO NET/Socket : Using [0]eno1:117.17.185.208<0>
seoultech:1703289:1703343 [2] NCCL INFO Using network Socket
seoultech:1703289:1703343 [2] NCCL INFO comm 0x11fd3870 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId b1000 commId 0x58738cb6d14d8eca - Init START
seoultech:1703289:1703343 [2] NCCL INFO Setting affinity for GPU 2 to fff0,00fff000
seoultech:1703289:1703343 [2] NCCL INFO NVLS multicast support is not available on dev 2
seoultech:1703289:1703343 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
seoultech:1703289:1703343 [2] NCCL INFO P2P Chunksize set to 131072
seoultech:1703289:1703343 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
seoultech:1703289:1703343 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct
seoultech:1703289:1703343 [2] NCCL INFO Connected all rings
seoultech:1703289:1703343 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct
seoultech:1703289:1703343 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct
seoultech:1703289:1703343 [2] NCCL INFO Connected all trees
seoultech:1703289:1703343 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
seoultech:1703289:1703343 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
seoultech:1703289:1703343 [2] NCCL INFO comm 0x11fd3870 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId b1000 commId 0x58738cb6d14d8eca - Init COMPLETE
seoultech:1703290:1703290 [3] NCCL INFO cudaDriverVersion 12020
seoultech:1703290:1703290 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703290:1703290 [3] NCCL INFO Bootstrap : Using eno1:117.17.185.208<0>
seoultech:1703290:1703290 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
seoultech:1703290:1703290 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
seoultech:1703290:1703342 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703290:1703342 [3] NCCL INFO NET/IB : No device found.
seoultech:1703290:1703342 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703290:1703342 [3] NCCL INFO NET/Socket : Using [0]eno1:117.17.185.208<0>
seoultech:1703290:1703342 [3] NCCL INFO Using network Socket
seoultech:1703290:1703342 [3] NCCL INFO comm 0x17906780 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId ca000 commId 0x58738cb6d14d8eca - Init START
seoultech:1703290:1703342 [3] NCCL INFO Setting affinity for GPU 3 to fff0,00fff000
seoultech:1703290:1703342 [3] NCCL INFO NVLS multicast support is not available on dev 3
seoultech:1703290:1703342 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
seoultech:1703290:1703342 [3] NCCL INFO P2P Chunksize set to 131072
seoultech:1703290:1703342 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
seoultech:1703290:1703342 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
seoultech:1703290:1703342 [3] NCCL INFO Connected all rings
seoultech:1703290:1703342 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct
seoultech:1703290:1703342 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct
seoultech:1703290:1703342 [3] NCCL INFO Connected all trees
seoultech:1703290:1703342 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
seoultech:1703290:1703342 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
seoultech:1703290:1703342 [3] NCCL INFO comm 0x17906780 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId ca000 commId 0x58738cb6d14d8eca - Init COMPLETE
seoultech:1703287:1703340 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703287:1703340 [0] NCCL INFO NET/IB : No device found.
seoultech:1703287:1703340 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703287:1703340 [0] NCCL INFO NET/Socket : Using [0]eno1:117.17.185.208<0>
seoultech:1703287:1703340 [0] NCCL INFO Using network Socket
seoultech:1703287:1703340 [0] NCCL INFO comm 0x233e25b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 31000 commId 0x58738cb6d14d8eca - Init START
seoultech:1703287:1703340 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff
seoultech:1703287:1703340 [0] NCCL INFO NVLS multicast support is not available on dev 0
seoultech:1703287:1703340 [0] NCCL INFO Channel 00/02 :    0   1   2   3
seoultech:1703287:1703340 [0] NCCL INFO Channel 01/02 :    0   1   2   3
seoultech:1703287:1703340 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
seoultech:1703287:1703340 [0] NCCL INFO P2P Chunksize set to 131072
seoultech:1703287:1703340 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
seoultech:1703287:1703340 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
seoultech:1703287:1703340 [0] NCCL INFO Connected all rings
seoultech:1703287:1703340 [0] NCCL INFO Connected all trees
seoultech:1703287:1703340 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
seoultech:1703287:1703340 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
seoultech:1703287:1703340 [0] NCCL INFO comm 0x233e25b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 31000 commId 0x58738cb6d14d8eca - Init COMPLETE
seoultech:1703288:1703288 [1] NCCL INFO cudaDriverVersion 12020
seoultech:1703288:1703288 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703288:1703288 [1] NCCL INFO Bootstrap : Using eno1:117.17.185.208<0>
seoultech:1703288:1703288 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
seoultech:1703288:1703288 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
seoultech:1703288:1703341 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703288:1703341 [1] NCCL INFO NET/IB : No device found.
seoultech:1703288:1703341 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,virbr,vmnet,vboxnet
seoultech:1703288:1703341 [1] NCCL INFO NET/Socket : Using [0]eno1:117.17.185.208<0>
seoultech:1703288:1703341 [1] NCCL INFO Using network Socket
seoultech:1703288:1703341 [1] NCCL INFO comm 0x42c80a40 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 4b000 commId 0x58738cb6d14d8eca - Init START
seoultech:1703288:1703341 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff
seoultech:1703288:1703341 [1] NCCL INFO NVLS multicast support is not available on dev 1
seoultech:1703288:1703341 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
seoultech:1703288:1703341 [1] NCCL INFO P2P Chunksize set to 131072
seoultech:1703288:1703341 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
seoultech:1703288:1703341 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
seoultech:1703288:1703341 [1] NCCL INFO Connected all rings
seoultech:1703288:1703341 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
seoultech:1703288:1703341 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
seoultech:1703288:1703341 [1] NCCL INFO Connected all trees
seoultech:1703288:1703341 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
seoultech:1703288:1703341 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
seoultech:1703288:1703341 [1] NCCL INFO comm 0x42c80a40 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 4b000 commId 0x58738cb6d14d8eca - Init COMPLETE
[seoultech:0/4] 2025-01-24 01:42:49,372 (trainer:754) INFO: 151epoch:train:1-10batch: iter_time=0.035, forward_time=0.097, loss=2.593, backward_time=0.056, grad_norm=0.338, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.021, optim0_lr0=0.001, train_time=1.356
[seoultech:0/4] 2025-01-24 01:42:51,109 (trainer:754) INFO: 151epoch:train:11-20batch: iter_time=2.016e-04, forward_time=0.056, loss=2.349, backward_time=0.049, grad_norm=0.393, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 01:42:52,979 (trainer:754) INFO: 151epoch:train:21-30batch: iter_time=3.406e-04, forward_time=0.058, loss=2.032, backward_time=0.049, grad_norm=0.412, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.373
[seoultech:0/4] 2025-01-24 01:42:54,709 (trainer:754) INFO: 151epoch:train:31-40batch: iter_time=2.380e-04, forward_time=0.051, loss=2.119, backward_time=0.048, grad_norm=0.340, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:42:56,450 (trainer:754) INFO: 151epoch:train:41-50batch: iter_time=1.778e-04, forward_time=0.053, loss=2.064, backward_time=0.049, grad_norm=0.439, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:42:58,156 (trainer:754) INFO: 151epoch:train:51-60batch: iter_time=1.766e-04, forward_time=0.048, loss=2.002, backward_time=0.048, grad_norm=0.441, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 01:42:59,862 (trainer:754) INFO: 151epoch:train:61-70batch: iter_time=1.317e-04, forward_time=0.048, loss=2.027, backward_time=0.050, grad_norm=0.434, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 01:43:01,800 (trainer:754) INFO: 151epoch:train:71-80batch: iter_time=2.011e-04, forward_time=0.056, loss=2.074, backward_time=0.053, grad_norm=0.416, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.387
[seoultech:0/4] 2025-01-24 01:43:03,462 (trainer:754) INFO: 151epoch:train:81-90batch: iter_time=2.144e-04, forward_time=0.047, loss=2.176, backward_time=0.047, grad_norm=0.437, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.332
[seoultech:0/4] 2025-01-24 01:43:05,159 (trainer:754) INFO: 151epoch:train:91-100batch: iter_time=1.194e-04, forward_time=0.048, loss=2.165, backward_time=0.046, grad_norm=0.428, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 01:43:06,854 (trainer:754) INFO: 151epoch:train:101-110batch: iter_time=1.393e-04, forward_time=0.048, loss=2.016, backward_time=0.046, grad_norm=0.492, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.339
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:43:47,402 (trainer:353) INFO: 151epoch results: [train] iter_time=0.003, forward_time=0.055, loss=2.134, backward_time=0.049, grad_norm=0.416, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.438, time=25.75 seconds, total_count=620710, gpu_max_cached_mem_GB=8.812, [valid] loss=1.772, time=5.5 seconds, total_count=924, gpu_max_cached_mem_GB=8.812, [att_plot] time=33.61 seconds, total_count=0, gpu_max_cached_mem_GB=8.812
[seoultech:0/4] 2025-01-24 01:43:50,695 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:43:50,697 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/144epoch.pth
[seoultech:0/4] 2025-01-24 01:43:50,697 (trainer:287) INFO: 152/200epoch started. Estimated time to finish: 55 minutes and 41.89 seconds
[seoultech:0/4] 2025-01-24 01:43:55,674 (trainer:754) INFO: 152epoch:train:1-10batch: iter_time=0.019, forward_time=0.052, loss=2.078, backward_time=0.048, grad_norm=0.491, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.994
[seoultech:0/4] 2025-01-24 01:43:57,437 (trainer:754) INFO: 152epoch:train:11-20batch: iter_time=0.006, forward_time=0.055, loss=1.948, backward_time=0.048, grad_norm=0.323, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 01:43:59,561 (trainer:754) INFO: 152epoch:train:21-30batch: iter_time=0.031, forward_time=0.061, loss=1.802, backward_time=0.046, grad_norm=0.402, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.424
[seoultech:0/4] 2025-01-24 01:44:01,282 (trainer:754) INFO: 152epoch:train:31-40batch: iter_time=2.036e-04, forward_time=0.051, loss=2.173, backward_time=0.048, grad_norm=0.379, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:44:03,024 (trainer:754) INFO: 152epoch:train:41-50batch: iter_time=1.920e-04, forward_time=0.052, loss=2.475, backward_time=0.048, grad_norm=0.324, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:44:04,779 (trainer:754) INFO: 152epoch:train:51-60batch: iter_time=1.752e-04, forward_time=0.054, loss=2.551, backward_time=0.048, grad_norm=0.321, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 01:44:06,488 (trainer:754) INFO: 152epoch:train:61-70batch: iter_time=2.366e-04, forward_time=0.051, loss=2.271, backward_time=0.048, grad_norm=0.380, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 01:44:08,261 (trainer:754) INFO: 152epoch:train:71-80batch: iter_time=1.570e-04, forward_time=0.058, loss=1.871, backward_time=0.047, grad_norm=0.340, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 01:44:10,003 (trainer:754) INFO: 152epoch:train:81-90batch: iter_time=1.865e-04, forward_time=0.054, loss=1.713, backward_time=0.047, grad_norm=0.301, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:44:11,748 (trainer:754) INFO: 152epoch:train:91-100batch: iter_time=2.053e-04, forward_time=0.056, loss=1.918, backward_time=0.047, grad_norm=0.320, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:44:13,809 (trainer:754) INFO: 152epoch:train:101-110batch: iter_time=2.022e-04, forward_time=0.085, loss=2.088, backward_time=0.047, grad_norm=0.321, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.412
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:44:54,420 (trainer:353) INFO: 152epoch results: [train] iter_time=0.005, forward_time=0.057, loss=2.091, backward_time=0.047, grad_norm=0.353, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.417, time=24.51 seconds, total_count=620824, gpu_max_cached_mem_GB=8.814, [valid] loss=1.743, time=5.1 seconds, total_count=931, gpu_max_cached_mem_GB=8.814, [att_plot] time=34.12 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:44:57,878 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:44:57,893 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/145epoch.pth
[seoultech:0/4] 2025-01-24 01:44:57,893 (trainer:287) INFO: 153/200epoch started. Estimated time to finish: 54 minutes and 9.56 seconds
[seoultech:0/4] 2025-01-24 01:45:03,355 (trainer:754) INFO: 153epoch:train:1-10batch: iter_time=0.032, forward_time=0.065, loss=2.204, backward_time=0.047, grad_norm=0.373, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=1.091
[seoultech:0/4] 2025-01-24 01:45:05,191 (trainer:754) INFO: 153epoch:train:11-20batch: iter_time=1.606e-04, forward_time=0.057, loss=2.129, backward_time=0.049, grad_norm=0.395, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.367
[seoultech:0/4] 2025-01-24 01:45:06,966 (trainer:754) INFO: 153epoch:train:21-30batch: iter_time=1.890e-04, forward_time=0.059, loss=2.040, backward_time=0.047, grad_norm=0.283, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 01:45:08,727 (trainer:754) INFO: 153epoch:train:31-40batch: iter_time=1.418e-04, forward_time=0.061, loss=1.720, backward_time=0.046, grad_norm=0.339, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 01:45:10,450 (trainer:754) INFO: 153epoch:train:41-50batch: iter_time=1.620e-04, forward_time=0.052, loss=2.010, backward_time=0.049, grad_norm=0.304, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:45:12,166 (trainer:754) INFO: 153epoch:train:51-60batch: iter_time=1.609e-04, forward_time=0.051, loss=1.984, backward_time=0.048, grad_norm=0.314, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 01:45:13,963 (trainer:754) INFO: 153epoch:train:61-70batch: iter_time=1.797e-04, forward_time=0.058, loss=2.388, backward_time=0.048, grad_norm=0.418, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.359
[seoultech:0/4] 2025-01-24 01:45:15,707 (trainer:754) INFO: 153epoch:train:71-80batch: iter_time=1.923e-04, forward_time=0.053, loss=2.079, backward_time=0.048, grad_norm=0.478, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:45:17,402 (trainer:754) INFO: 153epoch:train:81-90batch: iter_time=1.335e-04, forward_time=0.050, loss=2.060, backward_time=0.047, grad_norm=0.422, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 01:45:19,130 (trainer:754) INFO: 153epoch:train:91-100batch: iter_time=2.100e-04, forward_time=0.054, loss=1.791, backward_time=0.047, grad_norm=0.518, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:45:20,847 (trainer:754) INFO: 153epoch:train:101-110batch: iter_time=1.355e-04, forward_time=0.052, loss=2.243, backward_time=0.047, grad_norm=0.416, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:46:01,737 (trainer:353) INFO: 153epoch results: [train] iter_time=0.003, forward_time=0.055, loss=2.057, backward_time=0.047, grad_norm=0.387, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.414, time=24.33 seconds, total_count=620938, gpu_max_cached_mem_GB=8.814, [valid] loss=1.725, time=6.01 seconds, total_count=938, gpu_max_cached_mem_GB=8.814, [att_plot] time=33.5 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:46:05,207 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:46:05,225 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/88epoch.pth
[seoultech:0/4] 2025-01-24 01:46:05,226 (trainer:287) INFO: 154/200epoch started. Estimated time to finish: 52 minutes and 56.11 seconds
[seoultech:0/4] 2025-01-24 01:46:10,294 (trainer:754) INFO: 154epoch:train:1-10batch: iter_time=0.025, forward_time=0.060, loss=1.931, backward_time=0.049, grad_norm=0.370, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.016, optim0_lr0=0.001, train_time=1.012
[seoultech:0/4] 2025-01-24 01:46:12,201 (trainer:754) INFO: 154epoch:train:11-20batch: iter_time=1.375e-04, forward_time=0.058, loss=2.121, backward_time=0.047, grad_norm=0.354, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.381
[seoultech:0/4] 2025-01-24 01:46:13,947 (trainer:754) INFO: 154epoch:train:21-30batch: iter_time=1.783e-04, forward_time=0.051, loss=2.137, backward_time=0.050, grad_norm=0.290, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:46:15,690 (trainer:754) INFO: 154epoch:train:31-40batch: iter_time=1.339e-04, forward_time=0.058, loss=1.966, backward_time=0.047, grad_norm=0.383, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:46:17,421 (trainer:754) INFO: 154epoch:train:41-50batch: iter_time=1.387e-04, forward_time=0.057, loss=2.159, backward_time=0.046, grad_norm=0.309, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:46:19,134 (trainer:754) INFO: 154epoch:train:51-60batch: iter_time=1.391e-04, forward_time=0.052, loss=2.269, backward_time=0.048, grad_norm=0.489, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:46:20,857 (trainer:754) INFO: 154epoch:train:61-70batch: iter_time=1.412e-04, forward_time=0.051, loss=2.390, backward_time=0.048, grad_norm=0.447, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:46:22,602 (trainer:754) INFO: 154epoch:train:71-80batch: iter_time=1.497e-04, forward_time=0.055, loss=1.681, backward_time=0.047, grad_norm=0.333, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:46:24,409 (trainer:754) INFO: 154epoch:train:81-90batch: iter_time=0.003, forward_time=0.060, loss=1.940, backward_time=0.047, grad_norm=0.359, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.361
[seoultech:0/4] 2025-01-24 01:46:26,152 (trainer:754) INFO: 154epoch:train:91-100batch: iter_time=1.359e-04, forward_time=0.052, loss=1.998, backward_time=0.049, grad_norm=0.375, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:46:27,889 (trainer:754) INFO: 154epoch:train:101-110batch: iter_time=1.398e-04, forward_time=0.056, loss=1.628, backward_time=0.047, grad_norm=0.361, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:47:09,026 (trainer:353) INFO: 154epoch results: [train] iter_time=0.002, forward_time=0.055, loss=2.023, backward_time=0.048, grad_norm=0.366, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.409, time=24.09 seconds, total_count=621052, gpu_max_cached_mem_GB=8.814, [valid] loss=1.704, time=5.53 seconds, total_count=945, gpu_max_cached_mem_GB=8.814, [att_plot] time=34.17 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:47:12,305 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:47:12,306 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/146epoch.pth
[seoultech:0/4] 2025-01-24 01:47:12,307 (trainer:287) INFO: 155/200epoch started. Estimated time to finish: 51 minutes and 42.83 seconds
[seoultech:0/4] 2025-01-24 01:47:17,394 (trainer:754) INFO: 155epoch:train:1-10batch: iter_time=0.023, forward_time=0.057, loss=2.146, backward_time=0.050, grad_norm=0.259, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.016
[seoultech:0/4] 2025-01-24 01:47:19,302 (trainer:754) INFO: 155epoch:train:11-20batch: iter_time=0.002, forward_time=0.057, loss=1.793, backward_time=0.047, grad_norm=0.307, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.381
[seoultech:0/4] 2025-01-24 01:47:21,076 (trainer:754) INFO: 155epoch:train:21-30batch: iter_time=1.257e-04, forward_time=0.056, loss=2.034, backward_time=0.047, grad_norm=0.288, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 01:47:22,801 (trainer:754) INFO: 155epoch:train:31-40batch: iter_time=1.332e-04, forward_time=0.053, loss=1.605, backward_time=0.047, grad_norm=0.272, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:47:24,522 (trainer:754) INFO: 155epoch:train:41-50batch: iter_time=1.379e-04, forward_time=0.053, loss=1.901, backward_time=0.048, grad_norm=0.243, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:47:26,228 (trainer:754) INFO: 155epoch:train:51-60batch: iter_time=1.334e-04, forward_time=0.050, loss=2.269, backward_time=0.047, grad_norm=0.284, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 01:47:28,017 (trainer:754) INFO: 155epoch:train:61-70batch: iter_time=1.416e-04, forward_time=0.062, loss=1.807, backward_time=0.046, grad_norm=0.345, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.357
[seoultech:0/4] 2025-01-24 01:47:29,771 (trainer:754) INFO: 155epoch:train:71-80batch: iter_time=1.327e-04, forward_time=0.054, loss=2.007, backward_time=0.049, grad_norm=0.360, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 01:47:31,496 (trainer:754) INFO: 155epoch:train:81-90batch: iter_time=1.297e-04, forward_time=0.052, loss=1.687, backward_time=0.048, grad_norm=0.347, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:47:33,234 (trainer:754) INFO: 155epoch:train:91-100batch: iter_time=1.293e-04, forward_time=0.053, loss=2.206, backward_time=0.048, grad_norm=0.277, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 01:47:35,007 (trainer:754) INFO: 155epoch:train:101-110batch: iter_time=1.712e-04, forward_time=0.056, loss=2.327, backward_time=0.048, grad_norm=0.296, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.354
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:48:16,384 (trainer:353) INFO: 155epoch results: [train] iter_time=0.002, forward_time=0.055, loss=1.986, backward_time=0.048, grad_norm=0.298, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.410, time=24.14 seconds, total_count=621166, gpu_max_cached_mem_GB=8.814, [valid] loss=1.687, time=5.76 seconds, total_count=952, gpu_max_cached_mem_GB=8.814, [att_plot] time=34.17 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:48:19,698 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:48:19,700 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/147epoch.pth
[seoultech:0/4] 2025-01-24 01:48:19,701 (trainer:287) INFO: 156/200epoch started. Estimated time to finish: 50 minutes and 34.85 seconds
[seoultech:0/4] 2025-01-24 01:48:25,010 (trainer:754) INFO: 156epoch:train:1-10batch: iter_time=0.029, forward_time=0.055, loss=1.712, backward_time=0.050, grad_norm=0.297, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=1.061
[seoultech:0/4] 2025-01-24 01:48:26,743 (trainer:754) INFO: 156epoch:train:11-20batch: iter_time=1.328e-04, forward_time=0.053, loss=1.735, backward_time=0.047, grad_norm=0.229, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:48:28,483 (trainer:754) INFO: 156epoch:train:21-30batch: iter_time=1.307e-04, forward_time=0.052, loss=1.995, backward_time=0.049, grad_norm=0.225, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:48:30,232 (trainer:754) INFO: 156epoch:train:31-40batch: iter_time=1.362e-04, forward_time=0.057, loss=1.871, backward_time=0.047, grad_norm=0.289, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:48:31,981 (trainer:754) INFO: 156epoch:train:41-50batch: iter_time=1.331e-04, forward_time=0.056, loss=1.980, backward_time=0.047, grad_norm=0.359, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:48:33,789 (trainer:754) INFO: 156epoch:train:51-60batch: iter_time=1.550e-04, forward_time=0.058, loss=2.103, backward_time=0.049, grad_norm=0.378, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.361
[seoultech:0/4] 2025-01-24 01:48:35,546 (trainer:754) INFO: 156epoch:train:61-70batch: iter_time=1.300e-04, forward_time=0.056, loss=1.829, backward_time=0.047, grad_norm=0.381, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 01:48:37,273 (trainer:754) INFO: 156epoch:train:71-80batch: iter_time=1.431e-04, forward_time=0.052, loss=2.392, backward_time=0.049, grad_norm=0.280, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:48:39,031 (trainer:754) INFO: 156epoch:train:81-90batch: iter_time=1.302e-04, forward_time=0.057, loss=2.080, backward_time=0.048, grad_norm=0.279, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 01:48:40,735 (trainer:754) INFO: 156epoch:train:91-100batch: iter_time=1.571e-04, forward_time=0.051, loss=1.815, backward_time=0.046, grad_norm=0.237, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 01:48:42,512 (trainer:754) INFO: 156epoch:train:101-110batch: iter_time=0.002, forward_time=0.056, loss=2.120, backward_time=0.048, grad_norm=0.279, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.355
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:49:23,337 (trainer:353) INFO: 156epoch results: [train] iter_time=0.003, forward_time=0.055, loss=1.955, backward_time=0.048, grad_norm=0.294, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.412, time=24.2 seconds, total_count=621280, gpu_max_cached_mem_GB=8.814, [valid] loss=1.676, time=5.79 seconds, total_count=959, gpu_max_cached_mem_GB=8.814, [att_plot] time=33.64 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:49:26,670 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:49:26,685 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/89epoch.pth
[seoultech:0/4] 2025-01-24 01:49:26,685 (trainer:287) INFO: 157/200epoch started. Estimated time to finish: 49 minutes and 24.06 seconds
[seoultech:0/4] 2025-01-24 01:49:31,792 (trainer:754) INFO: 157epoch:train:1-10batch: iter_time=0.031, forward_time=0.061, loss=1.631, backward_time=0.047, grad_norm=0.256, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.020
[seoultech:0/4] 2025-01-24 01:49:33,554 (trainer:754) INFO: 157epoch:train:11-20batch: iter_time=1.339e-04, forward_time=0.056, loss=2.069, backward_time=0.048, grad_norm=0.349, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 01:49:35,276 (trainer:754) INFO: 157epoch:train:21-30batch: iter_time=1.270e-04, forward_time=0.051, loss=1.801, backward_time=0.048, grad_norm=0.268, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:49:37,023 (trainer:754) INFO: 157epoch:train:31-40batch: iter_time=1.382e-04, forward_time=0.053, loss=1.920, backward_time=0.048, grad_norm=0.255, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:49:38,819 (trainer:754) INFO: 157epoch:train:41-50batch: iter_time=1.863e-04, forward_time=0.058, loss=1.973, backward_time=0.047, grad_norm=0.324, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.359
[seoultech:0/4] 2025-01-24 01:49:40,522 (trainer:754) INFO: 157epoch:train:51-60batch: iter_time=1.555e-04, forward_time=0.052, loss=1.653, backward_time=0.047, grad_norm=0.289, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 01:49:42,166 (trainer:754) INFO: 157epoch:train:61-70batch: iter_time=1.421e-04, forward_time=0.049, loss=1.923, backward_time=0.046, grad_norm=0.275, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.329
[seoultech:0/4] 2025-01-24 01:49:43,930 (trainer:754) INFO: 157epoch:train:71-80batch: iter_time=2.007e-04, forward_time=0.056, loss=2.050, backward_time=0.049, grad_norm=0.285, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.353
[seoultech:0/4] 2025-01-24 01:49:45,646 (trainer:754) INFO: 157epoch:train:81-90batch: iter_time=1.403e-04, forward_time=0.051, loss=2.388, backward_time=0.048, grad_norm=0.287, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 01:49:47,396 (trainer:754) INFO: 157epoch:train:91-100batch: iter_time=1.444e-04, forward_time=0.057, loss=1.639, backward_time=0.047, grad_norm=0.293, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:49:49,119 (trainer:754) INFO: 157epoch:train:101-110batch: iter_time=1.414e-04, forward_time=0.050, loss=2.139, backward_time=0.048, grad_norm=0.326, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:50:29,738 (trainer:353) INFO: 157epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.927, backward_time=0.048, grad_norm=0.296, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.405, time=23.84 seconds, total_count=621394, gpu_max_cached_mem_GB=8.814, [valid] loss=1.661, time=5.43 seconds, total_count=966, gpu_max_cached_mem_GB=8.814, [att_plot] time=33.78 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:50:33,043 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:50:33,045 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/148epoch.pth
[seoultech:0/4] 2025-01-24 01:50:33,045 (trainer:287) INFO: 158/200epoch started. Estimated time to finish: 48 minutes and 10.52 seconds
[seoultech:0/4] 2025-01-24 01:50:38,489 (trainer:754) INFO: 158epoch:train:1-10batch: iter_time=0.028, forward_time=0.059, loss=2.004, backward_time=0.049, grad_norm=0.268, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.088
[seoultech:0/4] 2025-01-24 01:50:40,165 (trainer:754) INFO: 158epoch:train:11-20batch: iter_time=1.578e-04, forward_time=0.052, loss=1.991, backward_time=0.046, grad_norm=0.322, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.335
[seoultech:0/4] 2025-01-24 01:50:41,899 (trainer:754) INFO: 158epoch:train:21-30batch: iter_time=1.481e-04, forward_time=0.053, loss=1.712, backward_time=0.048, grad_norm=0.337, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:50:43,763 (trainer:754) INFO: 158epoch:train:31-40batch: iter_time=0.008, forward_time=0.057, loss=1.686, backward_time=0.048, grad_norm=0.312, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.373
[seoultech:0/4] 2025-01-24 01:50:45,490 (trainer:754) INFO: 158epoch:train:41-50batch: iter_time=1.577e-04, forward_time=0.053, loss=2.190, backward_time=0.048, grad_norm=0.266, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:50:47,323 (trainer:754) INFO: 158epoch:train:51-60batch: iter_time=2.086e-04, forward_time=0.064, loss=1.716, backward_time=0.047, grad_norm=0.287, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.366
[seoultech:0/4] 2025-01-24 01:50:49,036 (trainer:754) INFO: 158epoch:train:61-70batch: iter_time=1.887e-04, forward_time=0.052, loss=2.016, backward_time=0.047, grad_norm=0.294, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:50:50,795 (trainer:754) INFO: 158epoch:train:71-80batch: iter_time=1.309e-04, forward_time=0.056, loss=2.009, backward_time=0.048, grad_norm=0.249, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 01:50:52,517 (trainer:754) INFO: 158epoch:train:81-90batch: iter_time=1.292e-04, forward_time=0.051, loss=2.058, backward_time=0.050, grad_norm=0.221, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:50:54,249 (trainer:754) INFO: 158epoch:train:91-100batch: iter_time=1.345e-04, forward_time=0.054, loss=1.825, backward_time=0.047, grad_norm=0.305, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:50:56,002 (trainer:754) INFO: 158epoch:train:101-110batch: iter_time=2.236e-04, forward_time=0.055, loss=1.839, backward_time=0.047, grad_norm=0.305, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:51:38,310 (trainer:353) INFO: 158epoch results: [train] iter_time=0.003, forward_time=0.055, loss=1.900, backward_time=0.048, grad_norm=0.288, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.414, time=24.46 seconds, total_count=621508, gpu_max_cached_mem_GB=8.814, [valid] loss=1.652, time=5.84 seconds, total_count=973, gpu_max_cached_mem_GB=8.814, [att_plot] time=34.96 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:51:41,615 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:51:41,628 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/90epoch.pth
[seoultech:0/4] 2025-01-24 01:51:41,629 (trainer:287) INFO: 159/200epoch started. Estimated time to finish: 47 minutes and 10.45 seconds
[seoultech:0/4] 2025-01-24 01:51:46,669 (trainer:754) INFO: 159epoch:train:1-10batch: iter_time=0.021, forward_time=0.054, loss=1.896, backward_time=0.048, grad_norm=0.290, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.007
[seoultech:0/4] 2025-01-24 01:51:48,408 (trainer:754) INFO: 159epoch:train:11-20batch: iter_time=1.363e-04, forward_time=0.054, loss=1.950, backward_time=0.048, grad_norm=0.235, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:51:50,158 (trainer:754) INFO: 159epoch:train:21-30batch: iter_time=2.078e-04, forward_time=0.050, loss=1.894, backward_time=0.050, grad_norm=0.304, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:51:52,141 (trainer:754) INFO: 159epoch:train:31-40batch: iter_time=2.289e-04, forward_time=0.080, loss=1.945, backward_time=0.046, grad_norm=0.295, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.396
[seoultech:0/4] 2025-01-24 01:51:53,882 (trainer:754) INFO: 159epoch:train:41-50batch: iter_time=1.933e-04, forward_time=0.053, loss=1.623, backward_time=0.047, grad_norm=0.256, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:51:55,635 (trainer:754) INFO: 159epoch:train:51-60batch: iter_time=1.489e-04, forward_time=0.055, loss=1.915, backward_time=0.048, grad_norm=0.261, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:51:57,337 (trainer:754) INFO: 159epoch:train:61-70batch: iter_time=1.416e-04, forward_time=0.053, loss=1.591, backward_time=0.047, grad_norm=0.228, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 01:51:59,041 (trainer:754) INFO: 159epoch:train:71-80batch: iter_time=1.891e-04, forward_time=0.053, loss=1.975, backward_time=0.047, grad_norm=0.227, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 01:52:00,770 (trainer:754) INFO: 159epoch:train:81-90batch: iter_time=1.541e-04, forward_time=0.052, loss=2.145, backward_time=0.048, grad_norm=0.260, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:52:02,793 (trainer:754) INFO: 159epoch:train:91-100batch: iter_time=0.014, forward_time=0.064, loss=2.105, backward_time=0.049, grad_norm=0.350, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.404
[seoultech:0/4] 2025-01-24 01:52:04,530 (trainer:754) INFO: 159epoch:train:101-110batch: iter_time=1.314e-04, forward_time=0.055, loss=1.715, backward_time=0.047, grad_norm=0.373, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:52:45,367 (trainer:353) INFO: 159epoch results: [train] iter_time=0.003, forward_time=0.057, loss=1.874, backward_time=0.048, grad_norm=0.282, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.418, time=24.55 seconds, total_count=621622, gpu_max_cached_mem_GB=8.814, [valid] loss=1.635, time=5.6 seconds, total_count=980, gpu_max_cached_mem_GB=8.814, [att_plot] time=33.59 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:52:48,835 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:52:48,837 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/149epoch.pth
[seoultech:0/4] 2025-01-24 01:52:48,837 (trainer:287) INFO: 160/200epoch started. Estimated time to finish: 46 minutes and 2.22 seconds
[seoultech:0/4] 2025-01-24 01:52:54,196 (trainer:754) INFO: 160epoch:train:1-10batch: iter_time=0.020, forward_time=0.054, loss=1.745, backward_time=0.047, grad_norm=0.269, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.071
[seoultech:0/4] 2025-01-24 01:52:56,146 (trainer:754) INFO: 160epoch:train:11-20batch: iter_time=1.252e-04, forward_time=0.061, loss=1.700, backward_time=0.046, grad_norm=0.246, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.390
[seoultech:0/4] 2025-01-24 01:52:57,857 (trainer:754) INFO: 160epoch:train:21-30batch: iter_time=1.419e-04, forward_time=0.050, loss=1.831, backward_time=0.048, grad_norm=0.247, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:52:59,636 (trainer:754) INFO: 160epoch:train:31-40batch: iter_time=1.482e-04, forward_time=0.057, loss=1.728, backward_time=0.048, grad_norm=0.236, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 01:53:01,367 (trainer:754) INFO: 160epoch:train:41-50batch: iter_time=1.310e-04, forward_time=0.053, loss=1.629, backward_time=0.048, grad_norm=0.236, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:53:03,145 (trainer:754) INFO: 160epoch:train:51-60batch: iter_time=0.003, forward_time=0.057, loss=1.896, backward_time=0.047, grad_norm=0.341, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 01:53:04,883 (trainer:754) INFO: 160epoch:train:61-70batch: iter_time=1.387e-04, forward_time=0.055, loss=1.889, backward_time=0.048, grad_norm=0.295, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 01:53:06,604 (trainer:754) INFO: 160epoch:train:71-80batch: iter_time=1.326e-04, forward_time=0.053, loss=1.912, backward_time=0.048, grad_norm=0.306, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:53:08,349 (trainer:754) INFO: 160epoch:train:81-90batch: iter_time=1.275e-04, forward_time=0.056, loss=2.027, backward_time=0.048, grad_norm=0.323, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:53:10,051 (trainer:754) INFO: 160epoch:train:91-100batch: iter_time=1.552e-04, forward_time=0.050, loss=2.106, backward_time=0.048, grad_norm=0.276, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 01:53:11,757 (trainer:754) INFO: 160epoch:train:101-110batch: iter_time=1.390e-04, forward_time=0.053, loss=1.994, backward_time=0.048, grad_norm=0.239, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:53:53,797 (trainer:353) INFO: 160epoch results: [train] iter_time=0.002, forward_time=0.055, loss=1.851, backward_time=0.048, grad_norm=0.278, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.414, time=24.34 seconds, total_count=621736, gpu_max_cached_mem_GB=8.814, [valid] loss=1.621, time=5.95 seconds, total_count=987, gpu_max_cached_mem_GB=8.814, [att_plot] time=34.66 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:53:57,254 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:53:57,256 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/150epoch.pth
[seoultech:0/4] 2025-01-24 01:53:57,256 (trainer:287) INFO: 161/200epoch started. Estimated time to finish: 44 minutes and 59.04 seconds
[seoultech:0/4] 2025-01-24 01:54:02,746 (trainer:754) INFO: 161epoch:train:1-10batch: iter_time=0.030, forward_time=0.061, loss=2.020, backward_time=0.048, grad_norm=0.323, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.097
[seoultech:0/4] 2025-01-24 01:54:04,693 (trainer:754) INFO: 161epoch:train:11-20batch: iter_time=2.746e-04, forward_time=0.061, loss=1.991, backward_time=0.047, grad_norm=0.288, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.389
[seoultech:0/4] 2025-01-24 01:54:06,454 (trainer:754) INFO: 161epoch:train:21-30batch: iter_time=1.324e-04, forward_time=0.055, loss=1.711, backward_time=0.048, grad_norm=0.350, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 01:54:08,177 (trainer:754) INFO: 161epoch:train:31-40batch: iter_time=1.499e-04, forward_time=0.057, loss=1.618, backward_time=0.046, grad_norm=0.391, clip=0.000e+00, loss_scale=4.056e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:54:09,897 (trainer:754) INFO: 161epoch:train:41-50batch: iter_time=1.423e-04, forward_time=0.052, loss=1.960, backward_time=0.048, grad_norm=0.361, clip=0.000e+00, loss_scale=6.490e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:54:11,648 (trainer:754) INFO: 161epoch:train:51-60batch: iter_time=1.523e-04, forward_time=0.056, loss=1.822, backward_time=0.046, grad_norm=0.375, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:54:13,370 (trainer:754) INFO: 161epoch:train:61-70batch: iter_time=1.326e-04, forward_time=0.053, loss=1.403, backward_time=0.047, grad_norm=0.265, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:54:15,099 (trainer:754) INFO: 161epoch:train:71-80batch: iter_time=1.343e-04, forward_time=0.054, loss=1.836, backward_time=0.048, grad_norm=0.371, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:54:16,844 (trainer:754) INFO: 161epoch:train:81-90batch: iter_time=1.787e-04, forward_time=0.054, loss=1.600, backward_time=0.047, grad_norm=0.241, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:54:18,596 (trainer:754) INFO: 161epoch:train:91-100batch: iter_time=1.445e-04, forward_time=0.053, loss=1.998, backward_time=0.048, grad_norm=0.327, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:54:20,345 (trainer:754) INFO: 161epoch:train:101-110batch: iter_time=1.380e-04, forward_time=0.053, loss=2.224, backward_time=0.048, grad_norm=0.330, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:55:01,003 (trainer:353) INFO: 161epoch results: [train] iter_time=0.003, forward_time=0.055, loss=1.832, backward_time=0.047, grad_norm=0.327, clip=0.000e+00, loss_scale=6.547e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.417, time=24.48 seconds, total_count=621850, gpu_max_cached_mem_GB=8.814, [valid] loss=1.615, time=5.53 seconds, total_count=994, gpu_max_cached_mem_GB=8.814, [att_plot] time=33.73 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:55:04,444 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:55:04,480 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/151epoch.pth
[seoultech:0/4] 2025-01-24 01:55:04,481 (trainer:287) INFO: 162/200epoch started. Estimated time to finish: 43 minutes and 50.68 seconds
[seoultech:0/4] 2025-01-24 01:55:09,505 (trainer:754) INFO: 162epoch:train:1-10batch: iter_time=0.020, forward_time=0.053, loss=1.480, backward_time=0.047, grad_norm=0.271, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.004
[seoultech:0/4] 2025-01-24 01:55:11,231 (trainer:754) INFO: 162epoch:train:11-20batch: iter_time=1.352e-04, forward_time=0.050, loss=1.982, backward_time=0.050, grad_norm=0.308, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 01:55:12,948 (trainer:754) INFO: 162epoch:train:21-30batch: iter_time=1.906e-04, forward_time=0.054, loss=1.477, backward_time=0.047, grad_norm=0.263, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 01:55:14,855 (trainer:754) INFO: 162epoch:train:31-40batch: iter_time=0.005, forward_time=0.060, loss=1.722, backward_time=0.047, grad_norm=0.376, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.381
[seoultech:0/4] 2025-01-24 01:55:16,590 (trainer:754) INFO: 162epoch:train:41-50batch: iter_time=1.262e-04, forward_time=0.052, loss=1.646, backward_time=0.048, grad_norm=0.312, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 01:55:18,349 (trainer:754) INFO: 162epoch:train:51-60batch: iter_time=1.505e-04, forward_time=0.054, loss=1.846, backward_time=0.048, grad_norm=0.397, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 01:55:20,080 (trainer:754) INFO: 162epoch:train:61-70batch: iter_time=1.369e-04, forward_time=0.053, loss=2.200, backward_time=0.048, grad_norm=0.421, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:55:21,825 (trainer:754) INFO: 162epoch:train:71-80batch: iter_time=2.058e-04, forward_time=0.054, loss=1.657, backward_time=0.048, grad_norm=0.432, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:55:23,541 (trainer:754) INFO: 162epoch:train:81-90batch: iter_time=1.368e-04, forward_time=0.054, loss=2.077, backward_time=0.046, grad_norm=0.303, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 01:55:25,324 (trainer:754) INFO: 162epoch:train:91-100batch: iter_time=1.882e-04, forward_time=0.058, loss=2.137, backward_time=0.048, grad_norm=0.376, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.356
[seoultech:0/4] 2025-01-24 01:55:27,149 (trainer:754) INFO: 162epoch:train:101-110batch: iter_time=0.005, forward_time=0.060, loss=1.767, backward_time=0.046, grad_norm=0.298, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.365
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:56:08,868 (trainer:353) INFO: 162epoch results: [train] iter_time=0.003, forward_time=0.055, loss=1.813, backward_time=0.047, grad_norm=0.337, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.409, time=24.03 seconds, total_count=621964, gpu_max_cached_mem_GB=8.814, [valid] loss=1.603, time=5.95 seconds, total_count=1001, gpu_max_cached_mem_GB=8.814, [att_plot] time=34.41 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:56:12,199 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:56:12,221 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/152epoch.pth
[seoultech:0/4] 2025-01-24 01:56:12,221 (trainer:287) INFO: 163/200epoch started. Estimated time to finish: 42 minutes and 44.13 seconds
[seoultech:0/4] 2025-01-24 01:56:17,392 (trainer:754) INFO: 163epoch:train:1-10batch: iter_time=0.036, forward_time=0.059, loss=1.765, backward_time=0.047, grad_norm=0.257, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.033
[seoultech:0/4] 2025-01-24 01:56:19,103 (trainer:754) INFO: 163epoch:train:11-20batch: iter_time=1.407e-04, forward_time=0.052, loss=1.956, backward_time=0.047, grad_norm=0.296, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:56:20,835 (trainer:754) INFO: 163epoch:train:21-30batch: iter_time=1.232e-04, forward_time=0.050, loss=1.990, backward_time=0.050, grad_norm=0.238, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:56:22,571 (trainer:754) INFO: 163epoch:train:31-40batch: iter_time=1.400e-04, forward_time=0.055, loss=1.728, backward_time=0.048, grad_norm=0.245, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 01:56:24,275 (trainer:754) INFO: 163epoch:train:41-50batch: iter_time=1.300e-04, forward_time=0.051, loss=1.599, backward_time=0.047, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 01:56:26,169 (trainer:754) INFO: 163epoch:train:51-60batch: iter_time=0.011, forward_time=0.059, loss=2.080, backward_time=0.048, grad_norm=0.347, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.378
[seoultech:0/4] 2025-01-24 01:56:27,918 (trainer:754) INFO: 163epoch:train:61-70batch: iter_time=1.274e-04, forward_time=0.058, loss=1.575, backward_time=0.047, grad_norm=0.244, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:56:29,613 (trainer:754) INFO: 163epoch:train:71-80batch: iter_time=1.297e-04, forward_time=0.051, loss=1.909, backward_time=0.047, grad_norm=0.222, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 01:56:31,337 (trainer:754) INFO: 163epoch:train:81-90batch: iter_time=1.452e-04, forward_time=0.055, loss=1.715, backward_time=0.047, grad_norm=0.298, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:56:33,056 (trainer:754) INFO: 163epoch:train:91-100batch: iter_time=1.865e-04, forward_time=0.054, loss=1.795, backward_time=0.047, grad_norm=0.247, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:56:34,784 (trainer:754) INFO: 163epoch:train:101-110batch: iter_time=2.036e-04, forward_time=0.056, loss=1.571, backward_time=0.045, grad_norm=0.307, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:57:15,546 (trainer:353) INFO: 163epoch results: [train] iter_time=0.004, forward_time=0.055, loss=1.789, backward_time=0.047, grad_norm=0.270, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.408, time=23.97 seconds, total_count=622078, gpu_max_cached_mem_GB=8.814, [valid] loss=1.590, time=5.47 seconds, total_count=1008, gpu_max_cached_mem_GB=8.814, [att_plot] time=33.89 seconds, total_count=0, gpu_max_cached_mem_GB=8.814
[seoultech:0/4] 2025-01-24 01:57:19,112 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:57:19,150 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/153epoch.pth
[seoultech:0/4] 2025-01-24 01:57:19,150 (trainer:287) INFO: 164/200epoch started. Estimated time to finish: 41 minutes and 35.1 seconds
[seoultech:0/4] 2025-01-24 01:57:24,603 (trainer:754) INFO: 164epoch:train:1-10batch: iter_time=0.028, forward_time=0.060, loss=1.692, backward_time=0.046, grad_norm=0.293, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.089
[seoultech:0/4] 2025-01-24 01:57:26,310 (trainer:754) INFO: 164epoch:train:11-20batch: iter_time=1.343e-04, forward_time=0.054, loss=2.018, backward_time=0.046, grad_norm=0.336, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 01:57:28,020 (trainer:754) INFO: 164epoch:train:21-30batch: iter_time=1.379e-04, forward_time=0.052, loss=1.729, backward_time=0.047, grad_norm=0.341, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:57:29,722 (trainer:754) INFO: 164epoch:train:31-40batch: iter_time=1.513e-04, forward_time=0.052, loss=1.703, backward_time=0.047, grad_norm=0.425, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 01:57:31,481 (trainer:754) INFO: 164epoch:train:41-50batch: iter_time=1.280e-04, forward_time=0.055, loss=1.968, backward_time=0.049, grad_norm=0.278, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 01:57:33,231 (trainer:754) INFO: 164epoch:train:51-60batch: iter_time=0.003, forward_time=0.054, loss=1.584, backward_time=0.047, grad_norm=0.273, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:57:35,103 (trainer:754) INFO: 164epoch:train:61-70batch: iter_time=0.008, forward_time=0.057, loss=1.644, backward_time=0.048, grad_norm=0.323, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.374
[seoultech:0/4] 2025-01-24 01:57:36,840 (trainer:754) INFO: 164epoch:train:71-80batch: iter_time=1.467e-04, forward_time=0.053, loss=1.696, backward_time=0.049, grad_norm=0.360, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 01:57:38,562 (trainer:754) INFO: 164epoch:train:81-90batch: iter_time=1.241e-04, forward_time=0.054, loss=1.963, backward_time=0.047, grad_norm=0.332, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 01:57:40,278 (trainer:754) INFO: 164epoch:train:91-100batch: iter_time=1.378e-04, forward_time=0.053, loss=1.776, backward_time=0.047, grad_norm=0.390, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 01:57:42,035 (trainer:754) INFO: 164epoch:train:101-110batch: iter_time=1.878e-04, forward_time=0.057, loss=1.600, backward_time=0.046, grad_norm=0.318, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:58:22,626 (trainer:353) INFO: 164epoch results: [train] iter_time=0.004, forward_time=0.055, loss=1.774, backward_time=0.047, grad_norm=0.330, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.413, time=24.35 seconds, total_count=622192, gpu_max_cached_mem_GB=9.006, [valid] loss=1.583, time=5.72 seconds, total_count=1015, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.4 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 01:58:25,986 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:58:26,009 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/154epoch.pth
[seoultech:0/4] 2025-01-24 01:58:26,009 (trainer:287) INFO: 165/200epoch started. Estimated time to finish: 40 minutes and 26.18 seconds
[seoultech:0/4] 2025-01-24 01:58:31,104 (trainer:754) INFO: 165epoch:train:1-10batch: iter_time=0.023, forward_time=0.055, loss=1.571, backward_time=0.048, grad_norm=0.252, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.018
[seoultech:0/4] 2025-01-24 01:58:32,897 (trainer:754) INFO: 165epoch:train:11-20batch: iter_time=1.328e-04, forward_time=0.052, loss=1.874, backward_time=0.048, grad_norm=0.250, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.358
[seoultech:0/4] 2025-01-24 01:58:34,611 (trainer:754) INFO: 165epoch:train:21-30batch: iter_time=1.364e-04, forward_time=0.051, loss=1.661, backward_time=0.048, grad_norm=0.258, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:58:36,313 (trainer:754) INFO: 165epoch:train:31-40batch: iter_time=1.748e-04, forward_time=0.048, loss=1.868, backward_time=0.047, grad_norm=0.225, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 01:58:38,045 (trainer:754) INFO: 165epoch:train:41-50batch: iter_time=1.381e-04, forward_time=0.051, loss=1.865, backward_time=0.051, grad_norm=0.334, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:58:39,758 (trainer:754) INFO: 165epoch:train:51-60batch: iter_time=1.441e-04, forward_time=0.052, loss=1.875, backward_time=0.047, grad_norm=0.319, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:58:41,457 (trainer:754) INFO: 165epoch:train:61-70batch: iter_time=1.280e-04, forward_time=0.050, loss=1.663, backward_time=0.048, grad_norm=0.268, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 01:58:43,128 (trainer:754) INFO: 165epoch:train:71-80batch: iter_time=1.396e-04, forward_time=0.051, loss=1.511, backward_time=0.046, grad_norm=0.239, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.334
[seoultech:0/4] 2025-01-24 01:58:44,791 (trainer:754) INFO: 165epoch:train:81-90batch: iter_time=1.727e-04, forward_time=0.048, loss=1.926, backward_time=0.047, grad_norm=0.280, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.332
[seoultech:0/4] 2025-01-24 01:58:46,473 (trainer:754) INFO: 165epoch:train:91-100batch: iter_time=1.376e-04, forward_time=0.051, loss=1.744, backward_time=0.046, grad_norm=0.224, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.336
[seoultech:0/4] 2025-01-24 01:58:48,441 (trainer:754) INFO: 165epoch:train:101-110batch: iter_time=0.004, forward_time=0.056, loss=1.575, backward_time=0.048, grad_norm=0.282, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.393
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 01:59:28,954 (trainer:353) INFO: 165epoch results: [train] iter_time=0.003, forward_time=0.051, loss=1.752, backward_time=0.048, grad_norm=0.266, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.405, time=23.79 seconds, total_count=622306, gpu_max_cached_mem_GB=9.006, [valid] loss=1.574, time=5.02 seconds, total_count=1022, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.13 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 01:59:32,685 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 01:59:32,724 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/155epoch.pth
[seoultech:0/4] 2025-01-24 01:59:32,724 (trainer:287) INFO: 166/200epoch started. Estimated time to finish: 39 minutes and 17.2 seconds
[seoultech:0/4] 2025-01-24 01:59:38,007 (trainer:754) INFO: 166epoch:train:1-10batch: iter_time=0.038, forward_time=0.062, loss=1.984, backward_time=0.047, grad_norm=0.315, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.055
[seoultech:0/4] 2025-01-24 01:59:39,953 (trainer:754) INFO: 166epoch:train:11-20batch: iter_time=0.004, forward_time=0.059, loss=1.385, backward_time=0.048, grad_norm=0.229, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.389
[seoultech:0/4] 2025-01-24 01:59:41,699 (trainer:754) INFO: 166epoch:train:21-30batch: iter_time=1.956e-04, forward_time=0.056, loss=1.932, backward_time=0.047, grad_norm=0.267, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 01:59:43,413 (trainer:754) INFO: 166epoch:train:31-40batch: iter_time=1.635e-04, forward_time=0.051, loss=1.688, backward_time=0.049, grad_norm=0.303, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:59:45,124 (trainer:754) INFO: 166epoch:train:41-50batch: iter_time=2.277e-04, forward_time=0.053, loss=1.615, backward_time=0.047, grad_norm=0.238, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 01:59:46,856 (trainer:754) INFO: 166epoch:train:51-60batch: iter_time=1.428e-04, forward_time=0.053, loss=1.610, backward_time=0.048, grad_norm=0.190, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 01:59:48,607 (trainer:754) INFO: 166epoch:train:61-70batch: iter_time=1.422e-04, forward_time=0.056, loss=2.107, backward_time=0.048, grad_norm=0.216, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:59:50,348 (trainer:754) INFO: 166epoch:train:71-80batch: iter_time=2.260e-04, forward_time=0.055, loss=1.795, backward_time=0.048, grad_norm=0.243, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 01:59:52,097 (trainer:754) INFO: 166epoch:train:81-90batch: iter_time=1.878e-04, forward_time=0.058, loss=1.507, backward_time=0.047, grad_norm=0.262, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 01:59:53,766 (trainer:754) INFO: 166epoch:train:91-100batch: iter_time=1.429e-04, forward_time=0.049, loss=1.725, backward_time=0.048, grad_norm=0.258, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.333
[seoultech:0/4] 2025-01-24 01:59:55,489 (trainer:754) INFO: 166epoch:train:101-110batch: iter_time=1.612e-04, forward_time=0.054, loss=1.754, backward_time=0.047, grad_norm=0.231, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.344
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:00:36,298 (trainer:353) INFO: 166epoch results: [train] iter_time=0.004, forward_time=0.055, loss=1.735, backward_time=0.047, grad_norm=0.257, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.411, time=24.16 seconds, total_count=622420, gpu_max_cached_mem_GB=9.006, [valid] loss=1.569, time=5.77 seconds, total_count=1029, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.64 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:00:39,614 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:00:39,637 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/156epoch.pth
[seoultech:0/4] 2025-01-24 02:00:39,638 (trainer:287) INFO: 167/200epoch started. Estimated time to finish: 38 minutes and 8.93 seconds
[seoultech:0/4] 2025-01-24 02:00:44,588 (trainer:754) INFO: 167epoch:train:1-10batch: iter_time=0.021, forward_time=0.054, loss=1.993, backward_time=0.050, grad_norm=0.301, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.989
[seoultech:0/4] 2025-01-24 02:00:46,262 (trainer:754) INFO: 167epoch:train:11-20batch: iter_time=1.318e-04, forward_time=0.049, loss=1.920, backward_time=0.047, grad_norm=0.313, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.335
[seoultech:0/4] 2025-01-24 02:00:48,018 (trainer:754) INFO: 167epoch:train:21-30batch: iter_time=1.315e-04, forward_time=0.052, loss=1.418, backward_time=0.048, grad_norm=0.222, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:00:49,738 (trainer:754) INFO: 167epoch:train:31-40batch: iter_time=1.378e-04, forward_time=0.053, loss=1.455, backward_time=0.046, grad_norm=0.245, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:00:51,487 (trainer:754) INFO: 167epoch:train:41-50batch: iter_time=1.596e-04, forward_time=0.056, loss=1.569, backward_time=0.048, grad_norm=0.246, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:00:53,205 (trainer:754) INFO: 167epoch:train:51-60batch: iter_time=1.341e-04, forward_time=0.050, loss=1.750, backward_time=0.049, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:00:54,922 (trainer:754) INFO: 167epoch:train:61-70batch: iter_time=1.466e-04, forward_time=0.054, loss=1.636, backward_time=0.045, grad_norm=0.223, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:00:56,719 (trainer:754) INFO: 167epoch:train:71-80batch: iter_time=1.473e-04, forward_time=0.059, loss=1.732, backward_time=0.047, grad_norm=0.242, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.359
[seoultech:0/4] 2025-01-24 02:00:58,464 (trainer:754) INFO: 167epoch:train:81-90batch: iter_time=1.420e-04, forward_time=0.054, loss=1.760, backward_time=0.048, grad_norm=0.232, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:01:00,183 (trainer:754) INFO: 167epoch:train:91-100batch: iter_time=1.434e-04, forward_time=0.054, loss=1.526, backward_time=0.047, grad_norm=0.209, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:01:01,898 (trainer:754) INFO: 167epoch:train:101-110batch: iter_time=1.410e-04, forward_time=0.053, loss=1.901, backward_time=0.047, grad_norm=0.220, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:01:42,252 (trainer:353) INFO: 167epoch results: [train] iter_time=0.002, forward_time=0.053, loss=1.718, backward_time=0.048, grad_norm=0.246, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.402, time=23.72 seconds, total_count=622534, gpu_max_cached_mem_GB=9.006, [valid] loss=1.558, time=5.42 seconds, total_count=1036, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.47 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:01:45,588 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:01:45,611 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/157epoch.pth
[seoultech:0/4] 2025-01-24 02:01:45,611 (trainer:287) INFO: 168/200epoch started. Estimated time to finish: 36 minutes and 58.99 seconds
[seoultech:0/4] 2025-01-24 02:01:50,956 (trainer:754) INFO: 168epoch:train:1-10batch: iter_time=0.043, forward_time=0.063, loss=1.464, backward_time=0.046, grad_norm=0.362, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.068
[seoultech:0/4] 2025-01-24 02:01:53,002 (trainer:754) INFO: 168epoch:train:11-20batch: iter_time=1.331e-04, forward_time=0.061, loss=1.641, backward_time=0.046, grad_norm=0.265, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.409
[seoultech:0/4] 2025-01-24 02:01:54,704 (trainer:754) INFO: 168epoch:train:21-30batch: iter_time=1.645e-04, forward_time=0.052, loss=1.340, backward_time=0.048, grad_norm=0.252, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:01:56,421 (trainer:754) INFO: 168epoch:train:31-40batch: iter_time=1.469e-04, forward_time=0.053, loss=1.628, backward_time=0.046, grad_norm=0.237, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:01:58,126 (trainer:754) INFO: 168epoch:train:41-50batch: iter_time=1.558e-04, forward_time=0.049, loss=2.240, backward_time=0.048, grad_norm=0.258, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:02:00,108 (trainer:754) INFO: 168epoch:train:51-60batch: iter_time=1.592e-04, forward_time=0.057, loss=1.734, backward_time=0.048, grad_norm=0.292, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.396
[seoultech:0/4] 2025-01-24 02:02:02,068 (trainer:754) INFO: 168epoch:train:61-70batch: iter_time=1.480e-04, forward_time=0.053, loss=2.161, backward_time=0.049, grad_norm=0.228, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.392
[seoultech:0/4] 2025-01-24 02:02:03,818 (trainer:754) INFO: 168epoch:train:71-80batch: iter_time=1.375e-04, forward_time=0.053, loss=1.775, backward_time=0.047, grad_norm=0.247, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:02:05,548 (trainer:754) INFO: 168epoch:train:81-90batch: iter_time=2.100e-04, forward_time=0.053, loss=1.591, backward_time=0.048, grad_norm=0.227, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:02:07,412 (trainer:754) INFO: 168epoch:train:91-100batch: iter_time=0.006, forward_time=0.058, loss=1.751, backward_time=0.048, grad_norm=0.229, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.373
[seoultech:0/4] 2025-01-24 02:02:09,129 (trainer:754) INFO: 168epoch:train:101-110batch: iter_time=1.286e-04, forward_time=0.051, loss=1.466, backward_time=0.048, grad_norm=0.260, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:02:49,159 (trainer:353) INFO: 168epoch results: [train] iter_time=0.004, forward_time=0.055, loss=1.703, backward_time=0.047, grad_norm=0.257, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.424, time=24.87 seconds, total_count=622648, gpu_max_cached_mem_GB=9.006, [valid] loss=1.555, time=5.04 seconds, total_count=1043, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.64 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:02:52,536 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:02:52,558 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/158epoch.pth
[seoultech:0/4] 2025-01-24 02:02:52,558 (trainer:287) INFO: 169/200epoch started. Estimated time to finish: 35 minutes and 51.22 seconds
[seoultech:0/4] 2025-01-24 02:02:58,275 (trainer:754) INFO: 169epoch:train:1-10batch: iter_time=0.058, forward_time=0.069, loss=1.764, backward_time=0.049, grad_norm=0.223, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.016, optim0_lr0=0.001, train_time=1.142
[seoultech:0/4] 2025-01-24 02:03:00,335 (trainer:754) INFO: 169epoch:train:11-20batch: iter_time=0.007, forward_time=0.064, loss=1.813, backward_time=0.046, grad_norm=0.279, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.412
[seoultech:0/4] 2025-01-24 02:03:02,154 (trainer:754) INFO: 169epoch:train:21-30batch: iter_time=0.005, forward_time=0.058, loss=1.596, backward_time=0.047, grad_norm=0.290, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.363
[seoultech:0/4] 2025-01-24 02:03:03,919 (trainer:754) INFO: 169epoch:train:31-40batch: iter_time=1.374e-04, forward_time=0.058, loss=1.330, backward_time=0.047, grad_norm=0.236, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.353
[seoultech:0/4] 2025-01-24 02:03:05,631 (trainer:754) INFO: 169epoch:train:41-50batch: iter_time=1.446e-04, forward_time=0.051, loss=1.628, backward_time=0.048, grad_norm=0.299, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:03:07,379 (trainer:754) INFO: 169epoch:train:51-60batch: iter_time=1.373e-04, forward_time=0.053, loss=2.076, backward_time=0.048, grad_norm=0.301, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:03:09,088 (trainer:754) INFO: 169epoch:train:61-70batch: iter_time=1.387e-04, forward_time=0.053, loss=1.961, backward_time=0.047, grad_norm=0.234, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:03:10,842 (trainer:754) INFO: 169epoch:train:71-80batch: iter_time=1.401e-04, forward_time=0.055, loss=1.501, backward_time=0.048, grad_norm=0.285, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:03:12,540 (trainer:754) INFO: 169epoch:train:81-90batch: iter_time=1.604e-04, forward_time=0.050, loss=1.412, backward_time=0.047, grad_norm=0.238, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 02:03:14,246 (trainer:754) INFO: 169epoch:train:91-100batch: iter_time=1.390e-04, forward_time=0.052, loss=1.623, backward_time=0.047, grad_norm=0.217, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:03:15,993 (trainer:754) INFO: 169epoch:train:101-110batch: iter_time=1.453e-04, forward_time=0.052, loss=1.894, backward_time=0.049, grad_norm=0.223, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:03:56,505 (trainer:353) INFO: 169epoch results: [train] iter_time=0.006, forward_time=0.056, loss=1.688, backward_time=0.048, grad_norm=0.255, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.423, time=24.89 seconds, total_count=622762, gpu_max_cached_mem_GB=9.006, [valid] loss=1.545, time=5.6 seconds, total_count=1050, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.45 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:03:59,843 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:03:59,865 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/159epoch.pth
[seoultech:0/4] 2025-01-24 02:03:59,865 (trainer:287) INFO: 170/200epoch started. Estimated time to finish: 34 minutes and 44.13 seconds
[seoultech:0/4] 2025-01-24 02:04:05,142 (trainer:754) INFO: 170epoch:train:1-10batch: iter_time=0.035, forward_time=0.064, loss=1.652, backward_time=0.048, grad_norm=0.205, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.017, optim0_lr0=0.001, train_time=1.054
[seoultech:0/4] 2025-01-24 02:04:06,847 (trainer:754) INFO: 170epoch:train:11-20batch: iter_time=1.342e-04, forward_time=0.054, loss=1.535, backward_time=0.046, grad_norm=0.252, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:04:08,626 (trainer:754) INFO: 170epoch:train:21-30batch: iter_time=1.425e-04, forward_time=0.056, loss=1.987, backward_time=0.048, grad_norm=0.338, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:04:10,403 (trainer:754) INFO: 170epoch:train:31-40batch: iter_time=1.662e-04, forward_time=0.058, loss=1.521, backward_time=0.048, grad_norm=0.341, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:04:12,170 (trainer:754) INFO: 170epoch:train:41-50batch: iter_time=1.412e-04, forward_time=0.057, loss=1.723, backward_time=0.047, grad_norm=0.269, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.353
[seoultech:0/4] 2025-01-24 02:04:13,860 (trainer:754) INFO: 170epoch:train:51-60batch: iter_time=1.656e-04, forward_time=0.050, loss=1.905, backward_time=0.048, grad_norm=0.415, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.338
[seoultech:0/4] 2025-01-24 02:04:15,621 (trainer:754) INFO: 170epoch:train:61-70batch: iter_time=1.288e-04, forward_time=0.057, loss=1.474, backward_time=0.047, grad_norm=0.358, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:04:17,364 (trainer:754) INFO: 170epoch:train:71-80batch: iter_time=1.304e-04, forward_time=0.053, loss=1.851, backward_time=0.049, grad_norm=0.278, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:04:19,129 (trainer:754) INFO: 170epoch:train:81-90batch: iter_time=1.627e-04, forward_time=0.056, loss=1.463, backward_time=0.047, grad_norm=0.266, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.353
[seoultech:0/4] 2025-01-24 02:04:20,890 (trainer:754) INFO: 170epoch:train:91-100batch: iter_time=1.317e-04, forward_time=0.056, loss=1.760, backward_time=0.047, grad_norm=0.291, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:04:22,623 (trainer:754) INFO: 170epoch:train:101-110batch: iter_time=1.356e-04, forward_time=0.054, loss=1.753, backward_time=0.047, grad_norm=0.246, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:05:02,948 (trainer:353) INFO: 170epoch results: [train] iter_time=0.003, forward_time=0.056, loss=1.677, backward_time=0.048, grad_norm=0.293, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.411, time=24.14 seconds, total_count=622876, gpu_max_cached_mem_GB=9.006, [valid] loss=1.548, time=4.97 seconds, total_count=1057, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.97 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:05:06,268 (trainer:406) INFO: There are no improvements in this epoch
[seoultech:0/4] 2025-01-24 02:05:06,292 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/160epoch.pth
[seoultech:0/4] 2025-01-24 02:05:06,292 (trainer:287) INFO: 171/200epoch started. Estimated time to finish: 33 minutes and 35.7 seconds
[seoultech:0/4] 2025-01-24 02:05:11,373 (trainer:754) INFO: 171epoch:train:1-10batch: iter_time=0.033, forward_time=0.055, loss=1.788, backward_time=0.046, grad_norm=0.293, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.015
[seoultech:0/4] 2025-01-24 02:05:13,117 (trainer:754) INFO: 171epoch:train:11-20batch: iter_time=2.384e-04, forward_time=0.052, loss=1.839, backward_time=0.050, grad_norm=0.379, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:05:14,918 (trainer:754) INFO: 171epoch:train:21-30batch: iter_time=0.007, forward_time=0.055, loss=1.763, backward_time=0.046, grad_norm=0.303, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.360
[seoultech:0/4] 2025-01-24 02:05:16,648 (trainer:754) INFO: 171epoch:train:31-40batch: iter_time=0.002, forward_time=0.052, loss=1.493, backward_time=0.048, grad_norm=0.264, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:05:18,516 (trainer:754) INFO: 171epoch:train:41-50batch: iter_time=0.010, forward_time=0.057, loss=1.584, backward_time=0.046, grad_norm=0.258, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.373
[seoultech:0/4] 2025-01-24 02:05:20,268 (trainer:754) INFO: 171epoch:train:51-60batch: iter_time=1.688e-04, forward_time=0.054, loss=1.816, backward_time=0.047, grad_norm=0.289, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:05:21,990 (trainer:754) INFO: 171epoch:train:61-70batch: iter_time=2.010e-04, forward_time=0.051, loss=1.517, backward_time=0.048, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:05:23,748 (trainer:754) INFO: 171epoch:train:71-80batch: iter_time=1.357e-04, forward_time=0.056, loss=1.749, backward_time=0.047, grad_norm=0.292, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:05:25,494 (trainer:754) INFO: 171epoch:train:81-90batch: iter_time=1.566e-04, forward_time=0.053, loss=1.295, backward_time=0.046, grad_norm=0.241, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:05:27,201 (trainer:754) INFO: 171epoch:train:91-100batch: iter_time=1.663e-04, forward_time=0.051, loss=1.814, backward_time=0.048, grad_norm=0.332, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:05:28,927 (trainer:754) INFO: 171epoch:train:101-110batch: iter_time=1.188e-04, forward_time=0.053, loss=1.748, backward_time=0.047, grad_norm=0.338, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:06:09,476 (trainer:353) INFO: 171epoch results: [train] iter_time=0.005, forward_time=0.053, loss=1.664, backward_time=0.047, grad_norm=0.292, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.408, time=23.99 seconds, total_count=622990, gpu_max_cached_mem_GB=9.006, [valid] loss=1.534, time=5.25 seconds, total_count=1064, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.94 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:06:12,814 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:06:12,837 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/161epoch.pth
[seoultech:0/4] 2025-01-24 02:06:12,838 (trainer:287) INFO: 172/200epoch started. Estimated time to finish: 32 minutes and 27.62 seconds
[seoultech:0/4] 2025-01-24 02:06:18,353 (trainer:754) INFO: 172epoch:train:1-10batch: iter_time=0.029, forward_time=0.056, loss=1.655, backward_time=0.047, grad_norm=0.203, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.102
[seoultech:0/4] 2025-01-24 02:06:20,037 (trainer:754) INFO: 172epoch:train:11-20batch: iter_time=1.792e-04, forward_time=0.052, loss=2.054, backward_time=0.046, grad_norm=0.307, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.337
[seoultech:0/4] 2025-01-24 02:06:21,870 (trainer:754) INFO: 172epoch:train:21-30batch: iter_time=0.001, forward_time=0.055, loss=1.728, backward_time=0.048, grad_norm=0.232, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.366
[seoultech:0/4] 2025-01-24 02:06:23,586 (trainer:754) INFO: 172epoch:train:31-40batch: iter_time=1.393e-04, forward_time=0.054, loss=1.438, backward_time=0.047, grad_norm=0.285, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:06:25,326 (trainer:754) INFO: 172epoch:train:41-50batch: iter_time=1.354e-04, forward_time=0.054, loss=1.820, backward_time=0.048, grad_norm=0.308, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:06:27,074 (trainer:754) INFO: 172epoch:train:51-60batch: iter_time=1.754e-04, forward_time=0.053, loss=1.628, backward_time=0.049, grad_norm=0.396, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:06:28,816 (trainer:754) INFO: 172epoch:train:61-70batch: iter_time=1.285e-04, forward_time=0.054, loss=1.608, backward_time=0.049, grad_norm=0.402, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:06:30,507 (trainer:754) INFO: 172epoch:train:71-80batch: iter_time=1.261e-04, forward_time=0.050, loss=1.916, backward_time=0.048, grad_norm=0.391, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.338
[seoultech:0/4] 2025-01-24 02:06:32,224 (trainer:754) INFO: 172epoch:train:81-90batch: iter_time=2.102e-04, forward_time=0.055, loss=1.568, backward_time=0.045, grad_norm=0.331, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:06:33,993 (trainer:754) INFO: 172epoch:train:91-100batch: iter_time=1.311e-04, forward_time=0.059, loss=1.501, backward_time=0.047, grad_norm=0.315, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.353
[seoultech:0/4] 2025-01-24 02:06:35,741 (trainer:754) INFO: 172epoch:train:101-110batch: iter_time=1.607e-04, forward_time=0.055, loss=1.255, backward_time=0.046, grad_norm=0.197, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:07:17,272 (trainer:353) INFO: 172epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.652, backward_time=0.047, grad_norm=0.303, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.413, time=24.31 seconds, total_count=623104, gpu_max_cached_mem_GB=9.006, [valid] loss=1.527, time=5.82 seconds, total_count=1071, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.31 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:07:20,740 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:07:20,780 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/162epoch.pth
[seoultech:0/4] 2025-01-24 02:07:20,780 (trainer:287) INFO: 173/200epoch started. Estimated time to finish: 31 minutes and 21.45 seconds
[seoultech:0/4] 2025-01-24 02:07:25,909 (trainer:754) INFO: 173epoch:train:1-10batch: iter_time=0.028, forward_time=0.055, loss=1.539, backward_time=0.048, grad_norm=0.205, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.024
[seoultech:0/4] 2025-01-24 02:07:27,634 (trainer:754) INFO: 173epoch:train:11-20batch: iter_time=2.026e-04, forward_time=0.053, loss=1.823, backward_time=0.047, grad_norm=0.193, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:07:29,342 (trainer:754) INFO: 173epoch:train:21-30batch: iter_time=1.411e-04, forward_time=0.050, loss=1.797, backward_time=0.048, grad_norm=0.238, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:07:31,077 (trainer:754) INFO: 173epoch:train:31-40batch: iter_time=1.452e-04, forward_time=0.051, loss=1.602, backward_time=0.048, grad_norm=0.209, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:07:32,838 (trainer:754) INFO: 173epoch:train:41-50batch: iter_time=1.715e-04, forward_time=0.055, loss=1.564, backward_time=0.047, grad_norm=0.226, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:07:34,600 (trainer:754) INFO: 173epoch:train:51-60batch: iter_time=1.451e-04, forward_time=0.054, loss=1.651, backward_time=0.048, grad_norm=0.292, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:07:36,413 (trainer:754) INFO: 173epoch:train:61-70batch: iter_time=1.457e-04, forward_time=0.061, loss=1.980, backward_time=0.047, grad_norm=0.307, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.362
[seoultech:0/4] 2025-01-24 02:07:38,149 (trainer:754) INFO: 173epoch:train:71-80batch: iter_time=1.267e-04, forward_time=0.055, loss=1.355, backward_time=0.047, grad_norm=0.281, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:07:39,875 (trainer:754) INFO: 173epoch:train:81-90batch: iter_time=1.405e-04, forward_time=0.054, loss=1.491, backward_time=0.047, grad_norm=0.259, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:07:41,631 (trainer:754) INFO: 173epoch:train:91-100batch: iter_time=1.369e-04, forward_time=0.053, loss=1.691, backward_time=0.048, grad_norm=0.246, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:07:43,359 (trainer:754) INFO: 173epoch:train:101-110batch: iter_time=1.719e-04, forward_time=0.055, loss=1.577, backward_time=0.046, grad_norm=0.216, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:08:24,346 (trainer:353) INFO: 173epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.636, backward_time=0.047, grad_norm=0.242, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.408, time=23.98 seconds, total_count=623218, gpu_max_cached_mem_GB=9.006, [valid] loss=1.523, time=5.05 seconds, total_count=1078, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.53 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:08:28,080 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:08:28,104 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/163epoch.pth
[seoultech:0/4] 2025-01-24 02:08:28,105 (trainer:287) INFO: 174/200epoch started. Estimated time to finish: 30 minutes and 14.41 seconds
[seoultech:0/4] 2025-01-24 02:08:33,136 (trainer:754) INFO: 174epoch:train:1-10batch: iter_time=0.026, forward_time=0.057, loss=1.482, backward_time=0.048, grad_norm=0.200, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.005
[seoultech:0/4] 2025-01-24 02:08:34,894 (trainer:754) INFO: 174epoch:train:11-20batch: iter_time=1.149e-04, forward_time=0.051, loss=1.900, backward_time=0.049, grad_norm=0.186, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:08:36,667 (trainer:754) INFO: 174epoch:train:21-30batch: iter_time=1.181e-04, forward_time=0.054, loss=1.635, backward_time=0.050, grad_norm=0.229, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 02:08:38,384 (trainer:754) INFO: 174epoch:train:31-40batch: iter_time=1.645e-04, forward_time=0.052, loss=1.740, backward_time=0.048, grad_norm=0.220, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:08:40,125 (trainer:754) INFO: 174epoch:train:41-50batch: iter_time=1.187e-04, forward_time=0.053, loss=1.774, backward_time=0.048, grad_norm=0.209, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:08:41,858 (trainer:754) INFO: 174epoch:train:51-60batch: iter_time=1.727e-04, forward_time=0.053, loss=1.426, backward_time=0.048, grad_norm=0.202, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:08:43,579 (trainer:754) INFO: 174epoch:train:61-70batch: iter_time=1.428e-04, forward_time=0.055, loss=1.595, backward_time=0.047, grad_norm=0.213, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:08:45,586 (trainer:754) INFO: 174epoch:train:71-80batch: iter_time=1.294e-04, forward_time=0.059, loss=1.404, backward_time=0.047, grad_norm=0.239, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.401
[seoultech:0/4] 2025-01-24 02:08:47,330 (trainer:754) INFO: 174epoch:train:81-90batch: iter_time=1.333e-04, forward_time=0.057, loss=1.897, backward_time=0.047, grad_norm=0.231, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:08:49,035 (trainer:754) INFO: 174epoch:train:91-100batch: iter_time=1.277e-04, forward_time=0.052, loss=1.742, backward_time=0.047, grad_norm=0.176, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:08:50,768 (trainer:754) INFO: 174epoch:train:101-110batch: iter_time=2.016e-04, forward_time=0.053, loss=1.303, backward_time=0.048, grad_norm=0.206, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:09:33,176 (trainer:353) INFO: 174epoch results: [train] iter_time=0.002, forward_time=0.054, loss=1.621, backward_time=0.048, grad_norm=0.211, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.409, time=24.09 seconds, total_count=623332, gpu_max_cached_mem_GB=9.006, [valid] loss=1.515, time=6 seconds, total_count=1085, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.98 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:09:36,733 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:09:36,757 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/164epoch.pth
[seoultech:0/4] 2025-01-24 02:09:36,757 (trainer:287) INFO: 175/200epoch started. Estimated time to finish: 29 minutes and 8.78 seconds
[seoultech:0/4] 2025-01-24 02:09:41,785 (trainer:754) INFO: 175epoch:train:1-10batch: iter_time=0.031, forward_time=0.055, loss=1.711, backward_time=0.048, grad_norm=0.212, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.005
[seoultech:0/4] 2025-01-24 02:09:43,695 (trainer:754) INFO: 175epoch:train:11-20batch: iter_time=0.003, forward_time=0.057, loss=1.326, backward_time=0.047, grad_norm=0.253, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.382
[seoultech:0/4] 2025-01-24 02:09:45,365 (trainer:754) INFO: 175epoch:train:21-30batch: iter_time=1.483e-04, forward_time=0.050, loss=1.582, backward_time=0.046, grad_norm=0.247, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.334
[seoultech:0/4] 2025-01-24 02:09:47,150 (trainer:754) INFO: 175epoch:train:31-40batch: iter_time=1.285e-04, forward_time=0.057, loss=1.527, backward_time=0.048, grad_norm=0.268, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.357
[seoultech:0/4] 2025-01-24 02:09:48,854 (trainer:754) INFO: 175epoch:train:41-50batch: iter_time=1.374e-04, forward_time=0.050, loss=1.320, backward_time=0.047, grad_norm=0.205, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:09:50,592 (trainer:754) INFO: 175epoch:train:51-60batch: iter_time=1.612e-04, forward_time=0.056, loss=1.519, backward_time=0.046, grad_norm=0.200, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.015, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:09:52,326 (trainer:754) INFO: 175epoch:train:61-70batch: iter_time=1.379e-04, forward_time=0.056, loss=1.893, backward_time=0.047, grad_norm=0.217, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:09:54,012 (trainer:754) INFO: 175epoch:train:71-80batch: iter_time=1.473e-04, forward_time=0.051, loss=1.663, backward_time=0.047, grad_norm=0.248, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.337
[seoultech:0/4] 2025-01-24 02:09:55,919 (trainer:754) INFO: 175epoch:train:81-90batch: iter_time=0.010, forward_time=0.058, loss=1.559, backward_time=0.048, grad_norm=0.288, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.381
[seoultech:0/4] 2025-01-24 02:09:57,666 (trainer:754) INFO: 175epoch:train:91-100batch: iter_time=1.390e-04, forward_time=0.054, loss=1.747, backward_time=0.048, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:09:59,451 (trainer:754) INFO: 175epoch:train:101-110batch: iter_time=1.311e-04, forward_time=0.057, loss=1.639, backward_time=0.049, grad_norm=0.254, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.357
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:10:40,719 (trainer:353) INFO: 175epoch results: [train] iter_time=0.004, forward_time=0.055, loss=1.611, backward_time=0.048, grad_norm=0.241, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.410, time=24.11 seconds, total_count=623446, gpu_max_cached_mem_GB=9.006, [valid] loss=1.503, time=5.79 seconds, total_count=1092, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.06 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:10:44,103 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:10:44,127 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/165epoch.pth
[seoultech:0/4] 2025-01-24 02:10:44,128 (trainer:287) INFO: 176/200epoch started. Estimated time to finish: 28 minutes and 1.63 seconds
[seoultech:0/4] 2025-01-24 02:10:49,057 (trainer:754) INFO: 176epoch:train:1-10batch: iter_time=0.024, forward_time=0.054, loss=1.670, backward_time=0.048, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.985
[seoultech:0/4] 2025-01-24 02:10:50,753 (trainer:754) INFO: 176epoch:train:11-20batch: iter_time=1.820e-04, forward_time=0.050, loss=1.788, backward_time=0.047, grad_norm=0.215, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 02:10:52,492 (trainer:754) INFO: 176epoch:train:21-30batch: iter_time=1.501e-04, forward_time=0.052, loss=1.680, backward_time=0.048, grad_norm=0.183, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:10:54,194 (trainer:754) INFO: 176epoch:train:31-40batch: iter_time=1.243e-04, forward_time=0.050, loss=1.526, backward_time=0.048, grad_norm=0.246, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:10:55,871 (trainer:754) INFO: 176epoch:train:41-50batch: iter_time=1.655e-04, forward_time=0.049, loss=1.570, backward_time=0.048, grad_norm=0.190, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.335
[seoultech:0/4] 2025-01-24 02:10:57,521 (trainer:754) INFO: 176epoch:train:51-60batch: iter_time=1.312e-04, forward_time=0.047, loss=1.552, backward_time=0.046, grad_norm=0.203, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.330
[seoultech:0/4] 2025-01-24 02:10:59,290 (trainer:754) INFO: 176epoch:train:61-70batch: iter_time=1.182e-04, forward_time=0.050, loss=1.290, backward_time=0.045, grad_norm=0.199, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 02:11:01,029 (trainer:754) INFO: 176epoch:train:71-80batch: iter_time=1.449e-04, forward_time=0.050, loss=1.649, backward_time=0.047, grad_norm=0.277, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:11:02,759 (trainer:754) INFO: 176epoch:train:81-90batch: iter_time=1.086e-04, forward_time=0.047, loss=1.760, backward_time=0.048, grad_norm=0.251, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:11:04,602 (trainer:754) INFO: 176epoch:train:91-100batch: iter_time=1.348e-04, forward_time=0.055, loss=1.348, backward_time=0.047, grad_norm=0.259, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.368
[seoultech:0/4] 2025-01-24 02:11:06,517 (trainer:754) INFO: 176epoch:train:101-110batch: iter_time=0.003, forward_time=0.066, loss=1.476, backward_time=0.047, grad_norm=0.278, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.383
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:11:48,136 (trainer:353) INFO: 176epoch results: [train] iter_time=0.003, forward_time=0.052, loss=1.599, backward_time=0.047, grad_norm=0.231, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.404, time=23.82 seconds, total_count=623560, gpu_max_cached_mem_GB=9.006, [valid] loss=1.499, time=5.37 seconds, total_count=1099, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.82 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:11:51,801 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:11:51,842 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/166epoch.pth
[seoultech:0/4] 2025-01-24 02:11:51,843 (trainer:287) INFO: 177/200epoch started. Estimated time to finish: 26 minutes and 54.78 seconds
[seoultech:0/4] 2025-01-24 02:11:57,563 (trainer:754) INFO: 177epoch:train:1-10batch: iter_time=0.028, forward_time=0.059, loss=1.380, backward_time=0.048, grad_norm=0.264, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.143
[seoultech:0/4] 2025-01-24 02:11:59,279 (trainer:754) INFO: 177epoch:train:11-20batch: iter_time=1.371e-04, forward_time=0.052, loss=2.141, backward_time=0.047, grad_norm=0.309, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:12:01,086 (trainer:754) INFO: 177epoch:train:21-30batch: iter_time=1.482e-04, forward_time=0.060, loss=1.525, backward_time=0.047, grad_norm=0.222, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.361
[seoultech:0/4] 2025-01-24 02:12:02,799 (trainer:754) INFO: 177epoch:train:31-40batch: iter_time=1.504e-04, forward_time=0.051, loss=1.511, backward_time=0.047, grad_norm=0.224, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:12:04,574 (trainer:754) INFO: 177epoch:train:41-50batch: iter_time=1.638e-04, forward_time=0.057, loss=1.581, backward_time=0.047, grad_norm=0.263, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:12:06,319 (trainer:754) INFO: 177epoch:train:51-60batch: iter_time=1.293e-04, forward_time=0.054, loss=1.324, backward_time=0.049, grad_norm=0.248, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:12:08,032 (trainer:754) INFO: 177epoch:train:61-70batch: iter_time=1.702e-04, forward_time=0.053, loss=1.724, backward_time=0.048, grad_norm=0.241, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:12:09,784 (trainer:754) INFO: 177epoch:train:71-80batch: iter_time=1.898e-04, forward_time=0.055, loss=1.501, backward_time=0.047, grad_norm=0.210, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:12:11,513 (trainer:754) INFO: 177epoch:train:81-90batch: iter_time=1.232e-04, forward_time=0.052, loss=1.882, backward_time=0.049, grad_norm=0.194, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:12:13,239 (trainer:754) INFO: 177epoch:train:91-100batch: iter_time=1.278e-04, forward_time=0.052, loss=1.670, backward_time=0.047, grad_norm=0.194, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:12:14,944 (trainer:754) INFO: 177epoch:train:101-110batch: iter_time=1.617e-04, forward_time=0.053, loss=1.390, backward_time=0.046, grad_norm=0.178, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:12:55,943 (trainer:353) INFO: 177epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.589, backward_time=0.047, grad_norm=0.230, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.417, time=24.54 seconds, total_count=623674, gpu_max_cached_mem_GB=9.006, [valid] loss=1.499, time=5.68 seconds, total_count=1106, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.88 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:12:59,299 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:12:59,322 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/167epoch.pth
[seoultech:0/4] 2025-01-24 02:12:59,322 (trainer:287) INFO: 178/200epoch started. Estimated time to finish: 25 minutes and 47.67 seconds
[seoultech:0/4] 2025-01-24 02:13:04,480 (trainer:754) INFO: 178epoch:train:1-10batch: iter_time=0.019, forward_time=0.059, loss=1.338, backward_time=0.047, grad_norm=0.204, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.031
[seoultech:0/4] 2025-01-24 02:13:06,386 (trainer:754) INFO: 178epoch:train:11-20batch: iter_time=1.187e-04, forward_time=0.057, loss=1.622, backward_time=0.049, grad_norm=0.214, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.381
[seoultech:0/4] 2025-01-24 02:13:08,056 (trainer:754) INFO: 178epoch:train:21-30batch: iter_time=1.379e-04, forward_time=0.049, loss=1.878, backward_time=0.046, grad_norm=0.214, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.334
[seoultech:0/4] 2025-01-24 02:13:09,916 (trainer:754) INFO: 178epoch:train:31-40batch: iter_time=0.007, forward_time=0.060, loss=1.614, backward_time=0.047, grad_norm=0.273, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.372
[seoultech:0/4] 2025-01-24 02:13:11,641 (trainer:754) INFO: 178epoch:train:41-50batch: iter_time=1.286e-04, forward_time=0.053, loss=1.862, backward_time=0.046, grad_norm=0.257, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:13:13,382 (trainer:754) INFO: 178epoch:train:51-60batch: iter_time=1.218e-04, forward_time=0.053, loss=1.502, backward_time=0.047, grad_norm=0.290, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:13:15,097 (trainer:754) INFO: 178epoch:train:61-70batch: iter_time=1.228e-04, forward_time=0.050, loss=1.547, backward_time=0.048, grad_norm=0.276, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:13:16,838 (trainer:754) INFO: 178epoch:train:71-80batch: iter_time=1.446e-04, forward_time=0.053, loss=1.380, backward_time=0.046, grad_norm=0.265, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:13:18,517 (trainer:754) INFO: 178epoch:train:81-90batch: iter_time=1.331e-04, forward_time=0.050, loss=1.643, backward_time=0.046, grad_norm=0.183, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.335
[seoultech:0/4] 2025-01-24 02:13:20,230 (trainer:754) INFO: 178epoch:train:91-100batch: iter_time=1.183e-04, forward_time=0.049, loss=1.474, backward_time=0.048, grad_norm=0.182, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:13:22,012 (trainer:754) INFO: 178epoch:train:101-110batch: iter_time=1.214e-04, forward_time=0.056, loss=1.579, backward_time=0.048, grad_norm=0.234, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.356
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:14:01,794 (trainer:353) INFO: 178epoch results: [train] iter_time=0.002, forward_time=0.054, loss=1.579, backward_time=0.047, grad_norm=0.236, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.410, time=24.16 seconds, total_count=623788, gpu_max_cached_mem_GB=9.006, [valid] loss=1.494, time=5.21 seconds, total_count=1113, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.1 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:14:05,150 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:14:05,174 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/168epoch.pth
[seoultech:0/4] 2025-01-24 02:14:05,174 (trainer:287) INFO: 179/200epoch started. Estimated time to finish: 24 minutes and 39.25 seconds
[seoultech:0/4] 2025-01-24 02:14:10,151 (trainer:754) INFO: 179epoch:train:1-10batch: iter_time=0.018, forward_time=0.057, loss=1.631, backward_time=0.048, grad_norm=0.177, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.994
[seoultech:0/4] 2025-01-24 02:14:11,864 (trainer:754) INFO: 179epoch:train:11-20batch: iter_time=1.521e-04, forward_time=0.052, loss=1.576, backward_time=0.048, grad_norm=0.202, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:14:13,616 (trainer:754) INFO: 179epoch:train:21-30batch: iter_time=0.007, forward_time=0.054, loss=1.188, backward_time=0.045, grad_norm=0.217, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:14:15,376 (trainer:754) INFO: 179epoch:train:31-40batch: iter_time=2.096e-04, forward_time=0.056, loss=1.648, backward_time=0.047, grad_norm=0.232, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:14:17,140 (trainer:754) INFO: 179epoch:train:41-50batch: iter_time=1.464e-04, forward_time=0.056, loss=1.585, backward_time=0.048, grad_norm=0.189, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:14:18,908 (trainer:754) INFO: 179epoch:train:51-60batch: iter_time=1.724e-04, forward_time=0.056, loss=1.754, backward_time=0.048, grad_norm=0.205, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.353
[seoultech:0/4] 2025-01-24 02:14:20,651 (trainer:754) INFO: 179epoch:train:61-70batch: iter_time=1.429e-04, forward_time=0.057, loss=1.327, backward_time=0.046, grad_norm=0.196, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:14:22,366 (trainer:754) INFO: 179epoch:train:71-80batch: iter_time=1.330e-04, forward_time=0.051, loss=1.631, backward_time=0.048, grad_norm=0.192, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:14:24,164 (trainer:754) INFO: 179epoch:train:81-90batch: iter_time=1.321e-04, forward_time=0.058, loss=1.715, backward_time=0.048, grad_norm=0.237, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.359
[seoultech:0/4] 2025-01-24 02:14:25,885 (trainer:754) INFO: 179epoch:train:91-100batch: iter_time=1.340e-04, forward_time=0.053, loss=1.551, backward_time=0.047, grad_norm=0.213, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:14:27,615 (trainer:754) INFO: 179epoch:train:101-110batch: iter_time=1.417e-04, forward_time=0.051, loss=1.686, backward_time=0.048, grad_norm=0.234, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:15:08,298 (trainer:353) INFO: 179epoch results: [train] iter_time=0.002, forward_time=0.055, loss=1.568, backward_time=0.047, grad_norm=0.209, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.405, time=23.81 seconds, total_count=623902, gpu_max_cached_mem_GB=9.006, [valid] loss=1.491, time=5.71 seconds, total_count=1120, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.6 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:15:11,660 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:15:11,683 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/170epoch.pth
[seoultech:0/4] 2025-01-24 02:15:11,683 (trainer:287) INFO: 180/200epoch started. Estimated time to finish: 23 minutes and 31.48 seconds
[seoultech:0/4] 2025-01-24 02:15:16,701 (trainer:754) INFO: 180epoch:train:1-10batch: iter_time=0.019, forward_time=0.050, loss=1.561, backward_time=0.048, grad_norm=0.255, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=1.003
[seoultech:0/4] 2025-01-24 02:15:18,663 (trainer:754) INFO: 180epoch:train:11-20batch: iter_time=0.003, forward_time=0.056, loss=1.559, backward_time=0.046, grad_norm=0.263, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.392
[seoultech:0/4] 2025-01-24 02:15:20,425 (trainer:754) INFO: 180epoch:train:21-30batch: iter_time=1.254e-04, forward_time=0.053, loss=1.879, backward_time=0.047, grad_norm=0.245, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:15:22,151 (trainer:754) INFO: 180epoch:train:31-40batch: iter_time=1.249e-04, forward_time=0.052, loss=1.576, backward_time=0.047, grad_norm=0.242, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:15:23,881 (trainer:754) INFO: 180epoch:train:41-50batch: iter_time=1.518e-04, forward_time=0.052, loss=1.587, backward_time=0.048, grad_norm=0.199, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:15:25,634 (trainer:754) INFO: 180epoch:train:51-60batch: iter_time=1.382e-04, forward_time=0.056, loss=1.512, backward_time=0.047, grad_norm=0.236, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:15:27,346 (trainer:754) INFO: 180epoch:train:61-70batch: iter_time=1.607e-04, forward_time=0.051, loss=1.398, backward_time=0.048, grad_norm=0.196, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:15:29,063 (trainer:754) INFO: 180epoch:train:71-80batch: iter_time=1.603e-04, forward_time=0.052, loss=1.544, backward_time=0.047, grad_norm=0.184, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:15:30,759 (trainer:754) INFO: 180epoch:train:81-90batch: iter_time=1.330e-04, forward_time=0.050, loss=1.548, backward_time=0.048, grad_norm=0.188, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 02:15:32,594 (trainer:754) INFO: 180epoch:train:91-100batch: iter_time=0.011, forward_time=0.057, loss=1.742, backward_time=0.046, grad_norm=0.237, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.367
[seoultech:0/4] 2025-01-24 02:15:34,332 (trainer:754) INFO: 180epoch:train:101-110batch: iter_time=1.366e-04, forward_time=0.055, loss=1.266, backward_time=0.048, grad_norm=0.242, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:16:15,450 (trainer:353) INFO: 180epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.559, backward_time=0.047, grad_norm=0.229, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.410, time=24.1 seconds, total_count=624016, gpu_max_cached_mem_GB=9.006, [valid] loss=1.481, time=5.66 seconds, total_count=1127, gpu_max_cached_mem_GB=9.006, [att_plot] time=34 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:16:18,811 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:16:18,835 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/169epoch.pth
[seoultech:0/4] 2025-01-24 02:16:18,835 (trainer:287) INFO: 181/200epoch started. Estimated time to finish: 22 minutes and 24.23 seconds
[seoultech:0/4] 2025-01-24 02:16:23,918 (trainer:754) INFO: 181epoch:train:1-10batch: iter_time=0.028, forward_time=0.058, loss=1.464, backward_time=0.047, grad_norm=0.220, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=1.015
[seoultech:0/4] 2025-01-24 02:16:25,626 (trainer:754) INFO: 181epoch:train:11-20batch: iter_time=2.232e-04, forward_time=0.051, loss=1.272, backward_time=0.048, grad_norm=0.227, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:16:27,353 (trainer:754) INFO: 181epoch:train:21-30batch: iter_time=1.358e-04, forward_time=0.052, loss=1.121, backward_time=0.047, grad_norm=0.189, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:16:29,072 (trainer:754) INFO: 181epoch:train:31-40batch: iter_time=1.878e-04, forward_time=0.052, loss=1.317, backward_time=0.048, grad_norm=0.209, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:16:30,816 (trainer:754) INFO: 181epoch:train:41-50batch: iter_time=1.886e-04, forward_time=0.054, loss=1.791, backward_time=0.047, grad_norm=0.214, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:16:32,623 (trainer:754) INFO: 181epoch:train:51-60batch: iter_time=1.321e-04, forward_time=0.058, loss=1.665, backward_time=0.050, grad_norm=0.240, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.361
[seoultech:0/4] 2025-01-24 02:16:34,338 (trainer:754) INFO: 181epoch:train:61-70batch: iter_time=1.337e-04, forward_time=0.053, loss=1.504, backward_time=0.047, grad_norm=0.176, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:16:36,093 (trainer:754) INFO: 181epoch:train:71-80batch: iter_time=1.929e-04, forward_time=0.055, loss=1.910, backward_time=0.046, grad_norm=0.219, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:16:37,834 (trainer:754) INFO: 181epoch:train:81-90batch: iter_time=1.491e-04, forward_time=0.056, loss=1.594, backward_time=0.046, grad_norm=0.185, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:16:39,610 (trainer:754) INFO: 181epoch:train:91-100batch: iter_time=1.235e-04, forward_time=0.055, loss=1.684, backward_time=0.049, grad_norm=0.204, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:16:41,324 (trainer:754) INFO: 181epoch:train:101-110batch: iter_time=1.544e-04, forward_time=0.054, loss=1.613, backward_time=0.046, grad_norm=0.184, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:17:22,308 (trainer:353) INFO: 181epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.548, backward_time=0.047, grad_norm=0.207, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.407, time=24.08 seconds, total_count=624130, gpu_max_cached_mem_GB=9.006, [valid] loss=1.476, time=5.55 seconds, total_count=1134, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.85 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:17:25,743 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:17:25,766 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/171epoch.pth
[seoultech:0/4] 2025-01-24 02:17:25,767 (trainer:287) INFO: 182/200epoch started. Estimated time to finish: 21 minutes and 16.84 seconds
[seoultech:0/4] 2025-01-24 02:17:30,602 (trainer:754) INFO: 182epoch:train:1-10batch: iter_time=0.018, forward_time=0.051, loss=1.533, backward_time=0.047, grad_norm=0.172, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.966
[seoultech:0/4] 2025-01-24 02:17:32,296 (trainer:754) INFO: 182epoch:train:11-20batch: iter_time=1.697e-04, forward_time=0.049, loss=1.781, backward_time=0.047, grad_norm=0.183, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 02:17:33,995 (trainer:754) INFO: 182epoch:train:21-30batch: iter_time=1.262e-04, forward_time=0.051, loss=1.141, backward_time=0.047, grad_norm=0.177, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:17:35,676 (trainer:754) INFO: 182epoch:train:31-40batch: iter_time=1.296e-04, forward_time=0.048, loss=1.734, backward_time=0.047, grad_norm=0.184, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.336
[seoultech:0/4] 2025-01-24 02:17:37,383 (trainer:754) INFO: 182epoch:train:41-50batch: iter_time=1.821e-04, forward_time=0.051, loss=1.616, backward_time=0.046, grad_norm=0.237, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:17:39,208 (trainer:754) INFO: 182epoch:train:51-60batch: iter_time=2.454e-04, forward_time=0.056, loss=1.269, backward_time=0.048, grad_norm=0.204, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.365
[seoultech:0/4] 2025-01-24 02:17:41,020 (trainer:754) INFO: 182epoch:train:61-70batch: iter_time=1.361e-04, forward_time=0.057, loss=1.284, backward_time=0.048, grad_norm=0.185, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.362
[seoultech:0/4] 2025-01-24 02:17:42,805 (trainer:754) INFO: 182epoch:train:71-80batch: iter_time=0.003, forward_time=0.057, loss=1.684, backward_time=0.048, grad_norm=0.234, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.357
[seoultech:0/4] 2025-01-24 02:17:44,539 (trainer:754) INFO: 182epoch:train:81-90batch: iter_time=1.366e-04, forward_time=0.051, loss=1.534, backward_time=0.050, grad_norm=0.227, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:17:46,476 (trainer:754) INFO: 182epoch:train:91-100batch: iter_time=0.011, forward_time=0.062, loss=1.593, backward_time=0.046, grad_norm=0.236, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.387
[seoultech:0/4] 2025-01-24 02:17:48,202 (trainer:754) INFO: 182epoch:train:101-110batch: iter_time=1.934e-04, forward_time=0.054, loss=1.633, backward_time=0.047, grad_norm=0.185, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:18:29,382 (trainer:353) INFO: 182epoch results: [train] iter_time=0.003, forward_time=0.053, loss=1.538, backward_time=0.047, grad_norm=0.200, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.405, time=23.87 seconds, total_count=624244, gpu_max_cached_mem_GB=9.006, [valid] loss=1.467, time=5.79 seconds, total_count=1141, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.95 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:18:32,745 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:18:32,770 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/172epoch.pth
[seoultech:0/4] 2025-01-24 02:18:32,770 (trainer:287) INFO: 183/200epoch started. Estimated time to finish: 20 minutes and 9.53 seconds
[seoultech:0/4] 2025-01-24 02:18:38,214 (trainer:754) INFO: 183epoch:train:1-10batch: iter_time=0.041, forward_time=0.061, loss=1.349, backward_time=0.047, grad_norm=0.215, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.088
[seoultech:0/4] 2025-01-24 02:18:39,884 (trainer:754) INFO: 183epoch:train:11-20batch: iter_time=1.280e-04, forward_time=0.050, loss=1.504, backward_time=0.048, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.334
[seoultech:0/4] 2025-01-24 02:18:41,616 (trainer:754) INFO: 183epoch:train:21-30batch: iter_time=1.589e-04, forward_time=0.056, loss=1.533, backward_time=0.047, grad_norm=0.275, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:18:43,318 (trainer:754) INFO: 183epoch:train:31-40batch: iter_time=1.358e-04, forward_time=0.054, loss=1.363, backward_time=0.046, grad_norm=0.285, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:18:45,072 (trainer:754) INFO: 183epoch:train:41-50batch: iter_time=1.301e-04, forward_time=0.053, loss=1.544, backward_time=0.048, grad_norm=0.329, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:18:46,871 (trainer:754) INFO: 183epoch:train:51-60batch: iter_time=1.280e-04, forward_time=0.059, loss=1.663, backward_time=0.048, grad_norm=0.383, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.359
[seoultech:0/4] 2025-01-24 02:18:48,590 (trainer:754) INFO: 183epoch:train:61-70batch: iter_time=1.366e-04, forward_time=0.053, loss=1.725, backward_time=0.047, grad_norm=0.307, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:18:50,311 (trainer:754) INFO: 183epoch:train:71-80batch: iter_time=1.363e-04, forward_time=0.052, loss=1.692, backward_time=0.047, grad_norm=0.246, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:18:52,062 (trainer:754) INFO: 183epoch:train:81-90batch: iter_time=1.562e-04, forward_time=0.056, loss=1.592, backward_time=0.047, grad_norm=0.231, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:18:53,807 (trainer:754) INFO: 183epoch:train:91-100batch: iter_time=1.312e-04, forward_time=0.055, loss=1.628, backward_time=0.048, grad_norm=0.187, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:18:55,557 (trainer:754) INFO: 183epoch:train:101-110batch: iter_time=1.489e-04, forward_time=0.053, loss=1.234, backward_time=0.048, grad_norm=0.181, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:19:36,810 (trainer:353) INFO: 183epoch results: [train] iter_time=0.004, forward_time=0.055, loss=1.533, backward_time=0.048, grad_norm=0.260, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.412, time=24.25 seconds, total_count=624358, gpu_max_cached_mem_GB=9.006, [valid] loss=1.466, time=5.18 seconds, total_count=1148, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.61 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:19:40,118 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:19:40,142 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/173epoch.pth
[seoultech:0/4] 2025-01-24 02:19:40,143 (trainer:287) INFO: 184/200epoch started. Estimated time to finish: 19 minutes and 2.42 seconds
[seoultech:0/4] 2025-01-24 02:19:45,279 (trainer:754) INFO: 184epoch:train:1-10batch: iter_time=0.020, forward_time=0.057, loss=1.560, backward_time=0.048, grad_norm=0.180, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.026
[seoultech:0/4] 2025-01-24 02:19:47,029 (trainer:754) INFO: 184epoch:train:11-20batch: iter_time=1.381e-04, forward_time=0.054, loss=1.600, backward_time=0.047, grad_norm=0.172, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.016, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:19:48,816 (trainer:754) INFO: 184epoch:train:21-30batch: iter_time=1.908e-04, forward_time=0.054, loss=1.366, backward_time=0.047, grad_norm=0.199, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.357
[seoultech:0/4] 2025-01-24 02:19:50,531 (trainer:754) INFO: 184epoch:train:31-40batch: iter_time=1.312e-04, forward_time=0.051, loss=1.747, backward_time=0.047, grad_norm=0.217, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:19:52,279 (trainer:754) INFO: 184epoch:train:41-50batch: iter_time=1.270e-04, forward_time=0.052, loss=1.708, backward_time=0.049, grad_norm=0.185, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:19:54,161 (trainer:754) INFO: 184epoch:train:51-60batch: iter_time=0.009, forward_time=0.062, loss=1.274, backward_time=0.046, grad_norm=0.202, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.376
[seoultech:0/4] 2025-01-24 02:19:55,875 (trainer:754) INFO: 184epoch:train:61-70batch: iter_time=1.438e-04, forward_time=0.051, loss=1.535, backward_time=0.048, grad_norm=0.193, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:19:57,633 (trainer:754) INFO: 184epoch:train:71-80batch: iter_time=1.782e-04, forward_time=0.059, loss=1.246, backward_time=0.045, grad_norm=0.211, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:19:59,389 (trainer:754) INFO: 184epoch:train:81-90batch: iter_time=1.522e-04, forward_time=0.056, loss=1.460, backward_time=0.048, grad_norm=0.229, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:20:01,098 (trainer:754) INFO: 184epoch:train:91-100batch: iter_time=1.607e-04, forward_time=0.051, loss=1.752, backward_time=0.048, grad_norm=0.191, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:20:02,860 (trainer:754) INFO: 184epoch:train:101-110batch: iter_time=1.381e-04, forward_time=0.051, loss=1.510, backward_time=0.049, grad_norm=0.201, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.352
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:20:44,399 (trainer:353) INFO: 184epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.520, backward_time=0.047, grad_norm=0.197, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.410, time=24.18 seconds, total_count=624472, gpu_max_cached_mem_GB=9.006, [valid] loss=1.461, time=5.25 seconds, total_count=1155, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.82 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:20:47,704 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:20:47,728 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/174epoch.pth
[seoultech:0/4] 2025-01-24 02:20:47,728 (trainer:287) INFO: 185/200epoch started. Estimated time to finish: 17 minutes and 55.4 seconds
[seoultech:0/4] 2025-01-24 02:20:52,736 (trainer:754) INFO: 185epoch:train:1-10batch: iter_time=0.028, forward_time=0.049, loss=1.344, backward_time=0.048, grad_norm=0.187, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=1.001
[seoultech:0/4] 2025-01-24 02:20:54,412 (trainer:754) INFO: 185epoch:train:11-20batch: iter_time=1.076e-04, forward_time=0.046, loss=1.509, backward_time=0.047, grad_norm=0.179, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.335
[seoultech:0/4] 2025-01-24 02:20:56,278 (trainer:754) INFO: 185epoch:train:21-30batch: iter_time=1.109e-04, forward_time=0.051, loss=1.324, backward_time=0.045, grad_norm=0.212, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.373
[seoultech:0/4] 2025-01-24 02:20:57,987 (trainer:754) INFO: 185epoch:train:31-40batch: iter_time=1.048e-04, forward_time=0.046, loss=1.579, backward_time=0.046, grad_norm=0.175, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:20:59,891 (trainer:754) INFO: 185epoch:train:41-50batch: iter_time=1.292e-04, forward_time=0.048, loss=1.225, backward_time=0.046, grad_norm=0.217, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.381
[seoultech:0/4] 2025-01-24 02:21:01,666 (trainer:754) INFO: 185epoch:train:51-60batch: iter_time=1.060e-04, forward_time=0.050, loss=1.667, backward_time=0.048, grad_norm=0.230, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:21:03,579 (trainer:754) INFO: 185epoch:train:61-70batch: iter_time=1.120e-04, forward_time=0.050, loss=1.759, backward_time=0.046, grad_norm=0.204, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.382
[seoultech:0/4] 2025-01-24 02:21:05,281 (trainer:754) INFO: 185epoch:train:71-80batch: iter_time=1.039e-04, forward_time=0.046, loss=1.262, backward_time=0.048, grad_norm=0.160, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:21:07,280 (trainer:754) INFO: 185epoch:train:81-90batch: iter_time=3.304e-04, forward_time=0.051, loss=1.622, backward_time=0.048, grad_norm=0.206, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.400
[seoultech:0/4] 2025-01-24 02:21:09,056 (trainer:754) INFO: 185epoch:train:91-100batch: iter_time=0.001, forward_time=0.052, loss=1.335, backward_time=0.048, grad_norm=0.215, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:21:10,763 (trainer:754) INFO: 185epoch:train:101-110batch: iter_time=1.100e-04, forward_time=0.047, loss=1.738, backward_time=0.048, grad_norm=0.177, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.341
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:21:51,505 (trainer:353) INFO: 185epoch results: [train] iter_time=0.003, forward_time=0.049, loss=1.511, backward_time=0.047, grad_norm=0.199, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.010, optim0_lr0=0.001, train_time=0.416, time=24.46 seconds, total_count=624586, gpu_max_cached_mem_GB=9.006, [valid] loss=1.454, time=5.72 seconds, total_count=1162, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.6 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:21:54,864 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:21:54,889 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/175epoch.pth
[seoultech:0/4] 2025-01-24 02:21:54,889 (trainer:287) INFO: 186/200epoch started. Estimated time to finish: 16 minutes and 48.17 seconds
[seoultech:0/4] 2025-01-24 02:22:00,228 (trainer:754) INFO: 186epoch:train:1-10batch: iter_time=0.028, forward_time=0.063, loss=1.338, backward_time=0.047, grad_norm=0.174, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.017, optim0_lr0=0.001, train_time=1.066
[seoultech:0/4] 2025-01-24 02:22:02,161 (trainer:754) INFO: 186epoch:train:11-20batch: iter_time=5.841e-04, forward_time=0.059, loss=1.437, backward_time=0.047, grad_norm=0.203, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.387
[seoultech:0/4] 2025-01-24 02:22:03,930 (trainer:754) INFO: 186epoch:train:21-30batch: iter_time=1.324e-04, forward_time=0.055, loss=1.513, backward_time=0.048, grad_norm=0.213, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 02:22:05,894 (trainer:754) INFO: 186epoch:train:31-40batch: iter_time=1.180e-04, forward_time=0.075, loss=1.809, backward_time=0.046, grad_norm=0.225, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.393
[seoultech:0/4] 2025-01-24 02:22:07,655 (trainer:754) INFO: 186epoch:train:41-50batch: iter_time=1.563e-04, forward_time=0.054, loss=1.130, backward_time=0.047, grad_norm=0.242, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:22:09,375 (trainer:754) INFO: 186epoch:train:51-60batch: iter_time=2.127e-04, forward_time=0.051, loss=1.388, backward_time=0.048, grad_norm=0.245, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:22:11,079 (trainer:754) INFO: 186epoch:train:61-70batch: iter_time=1.607e-04, forward_time=0.049, loss=1.757, backward_time=0.049, grad_norm=0.234, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:22:12,785 (trainer:754) INFO: 186epoch:train:71-80batch: iter_time=1.311e-04, forward_time=0.050, loss=1.292, backward_time=0.047, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:22:14,511 (trainer:754) INFO: 186epoch:train:81-90batch: iter_time=1.386e-04, forward_time=0.052, loss=1.811, backward_time=0.046, grad_norm=0.229, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:22:16,334 (trainer:754) INFO: 186epoch:train:91-100batch: iter_time=1.734e-04, forward_time=0.058, loss=1.703, backward_time=0.049, grad_norm=0.227, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.364
[seoultech:0/4] 2025-01-24 02:22:18,089 (trainer:754) INFO: 186epoch:train:101-110batch: iter_time=1.660e-04, forward_time=0.057, loss=1.507, backward_time=0.047, grad_norm=0.200, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:22:59,835 (trainer:353) INFO: 186epoch results: [train] iter_time=0.003, forward_time=0.056, loss=1.504, backward_time=0.047, grad_norm=0.223, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.418, time=24.59 seconds, total_count=624700, gpu_max_cached_mem_GB=9.006, [valid] loss=1.454, time=5.6 seconds, total_count=1169, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.75 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:23:03,732 (trainer:406) INFO: There are no improvements in this epoch
[seoultech:0/4] 2025-01-24 02:23:03,773 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/176epoch.pth
[seoultech:0/4] 2025-01-24 02:23:03,773 (trainer:287) INFO: 187/200epoch started. Estimated time to finish: 15 minutes and 41.61 seconds
[seoultech:0/4] 2025-01-24 02:23:08,918 (trainer:754) INFO: 187epoch:train:1-10batch: iter_time=0.026, forward_time=0.055, loss=1.352, backward_time=0.047, grad_norm=0.204, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=1.028
[seoultech:0/4] 2025-01-24 02:23:10,630 (trainer:754) INFO: 187epoch:train:11-20batch: iter_time=1.345e-04, forward_time=0.051, loss=1.460, backward_time=0.046, grad_norm=0.189, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:23:12,355 (trainer:754) INFO: 187epoch:train:21-30batch: iter_time=1.286e-04, forward_time=0.052, loss=1.804, backward_time=0.048, grad_norm=0.217, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:23:14,104 (trainer:754) INFO: 187epoch:train:31-40batch: iter_time=1.361e-04, forward_time=0.052, loss=1.475, backward_time=0.049, grad_norm=0.160, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:23:15,849 (trainer:754) INFO: 187epoch:train:41-50batch: iter_time=1.600e-04, forward_time=0.055, loss=1.457, backward_time=0.048, grad_norm=0.189, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:23:17,601 (trainer:754) INFO: 187epoch:train:51-60batch: iter_time=6.165e-04, forward_time=0.056, loss=1.589, backward_time=0.048, grad_norm=0.201, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:23:19,336 (trainer:754) INFO: 187epoch:train:61-70batch: iter_time=1.441e-04, forward_time=0.055, loss=1.560, backward_time=0.047, grad_norm=0.202, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:23:21,090 (trainer:754) INFO: 187epoch:train:71-80batch: iter_time=1.513e-04, forward_time=0.052, loss=1.513, backward_time=0.049, grad_norm=0.194, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:23:22,786 (trainer:754) INFO: 187epoch:train:81-90batch: iter_time=1.776e-04, forward_time=0.051, loss=1.772, backward_time=0.047, grad_norm=0.181, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 02:23:24,550 (trainer:754) INFO: 187epoch:train:91-100batch: iter_time=1.424e-04, forward_time=0.058, loss=1.211, backward_time=0.046, grad_norm=0.196, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.353
[seoultech:0/4] 2025-01-24 02:23:26,294 (trainer:754) INFO: 187epoch:train:101-110batch: iter_time=1.948e-04, forward_time=0.054, loss=1.390, backward_time=0.048, grad_norm=0.206, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:24:07,053 (trainer:353) INFO: 187epoch results: [train] iter_time=0.002, forward_time=0.054, loss=1.494, backward_time=0.047, grad_norm=0.195, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.407, time=23.88 seconds, total_count=624814, gpu_max_cached_mem_GB=9.006, [valid] loss=1.449, time=5.98 seconds, total_count=1176, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.41 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:24:10,533 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:24:10,557 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/177epoch.pth
[seoultech:0/4] 2025-01-24 02:24:10,558 (trainer:287) INFO: 188/200epoch started. Estimated time to finish: 14 minutes and 34.18 seconds
[seoultech:0/4] 2025-01-24 02:24:15,720 (trainer:754) INFO: 188epoch:train:1-10batch: iter_time=0.026, forward_time=0.062, loss=1.197, backward_time=0.046, grad_norm=0.177, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.031
[seoultech:0/4] 2025-01-24 02:24:17,479 (trainer:754) INFO: 188epoch:train:11-20batch: iter_time=1.250e-04, forward_time=0.054, loss=1.367, backward_time=0.046, grad_norm=0.179, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:24:19,200 (trainer:754) INFO: 188epoch:train:21-30batch: iter_time=2.019e-04, forward_time=0.052, loss=1.444, backward_time=0.047, grad_norm=0.161, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:24:21,264 (trainer:754) INFO: 188epoch:train:31-40batch: iter_time=0.003, forward_time=0.059, loss=1.593, backward_time=0.048, grad_norm=0.208, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.413
[seoultech:0/4] 2025-01-24 02:24:22,990 (trainer:754) INFO: 188epoch:train:41-50batch: iter_time=1.337e-04, forward_time=0.055, loss=1.432, backward_time=0.047, grad_norm=0.168, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:24:24,695 (trainer:754) INFO: 188epoch:train:51-60batch: iter_time=1.499e-04, forward_time=0.050, loss=1.668, backward_time=0.049, grad_norm=0.184, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:24:26,414 (trainer:754) INFO: 188epoch:train:61-70batch: iter_time=1.298e-04, forward_time=0.050, loss=1.349, backward_time=0.047, grad_norm=0.180, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:24:28,220 (trainer:754) INFO: 188epoch:train:71-80batch: iter_time=1.379e-04, forward_time=0.059, loss=1.567, backward_time=0.047, grad_norm=0.232, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.361
[seoultech:0/4] 2025-01-24 02:24:29,976 (trainer:754) INFO: 188epoch:train:81-90batch: iter_time=1.313e-04, forward_time=0.055, loss=1.423, backward_time=0.046, grad_norm=0.192, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:24:31,708 (trainer:754) INFO: 188epoch:train:91-100batch: iter_time=1.167e-04, forward_time=0.050, loss=1.457, backward_time=0.048, grad_norm=0.231, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:24:33,406 (trainer:754) INFO: 188epoch:train:101-110batch: iter_time=1.236e-04, forward_time=0.050, loss=1.519, backward_time=0.047, grad_norm=0.229, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.339
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:25:13,464 (trainer:353) INFO: 188epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.486, backward_time=0.047, grad_norm=0.195, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.413, time=24.3 seconds, total_count=624928, gpu_max_cached_mem_GB=9.006, [valid] loss=1.439, time=5.18 seconds, total_count=1183, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.42 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:25:17,005 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:25:17,029 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/178epoch.pth
[seoultech:0/4] 2025-01-24 02:25:17,029 (trainer:287) INFO: 189/200epoch started. Estimated time to finish: 13 minutes and 26.7 seconds
[seoultech:0/4] 2025-01-24 02:25:22,109 (trainer:754) INFO: 189epoch:train:1-10batch: iter_time=0.030, forward_time=0.059, loss=1.700, backward_time=0.050, grad_norm=0.263, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.015
[seoultech:0/4] 2025-01-24 02:25:24,082 (trainer:754) INFO: 189epoch:train:11-20batch: iter_time=0.003, forward_time=0.059, loss=1.286, backward_time=0.047, grad_norm=0.216, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.394
[seoultech:0/4] 2025-01-24 02:25:25,809 (trainer:754) INFO: 189epoch:train:21-30batch: iter_time=1.936e-04, forward_time=0.054, loss=1.469, backward_time=0.048, grad_norm=0.221, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:25:27,526 (trainer:754) INFO: 189epoch:train:31-40batch: iter_time=2.049e-04, forward_time=0.054, loss=1.643, backward_time=0.047, grad_norm=0.216, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:25:29,302 (trainer:754) INFO: 189epoch:train:41-50batch: iter_time=1.383e-04, forward_time=0.057, loss=1.385, backward_time=0.047, grad_norm=0.304, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:25:31,011 (trainer:754) INFO: 189epoch:train:51-60batch: iter_time=1.370e-04, forward_time=0.052, loss=1.541, backward_time=0.048, grad_norm=0.271, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:25:32,772 (trainer:754) INFO: 189epoch:train:61-70batch: iter_time=1.680e-04, forward_time=0.057, loss=1.481, backward_time=0.047, grad_norm=0.281, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:25:34,497 (trainer:754) INFO: 189epoch:train:71-80batch: iter_time=1.203e-04, forward_time=0.052, loss=1.520, backward_time=0.049, grad_norm=0.251, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:25:36,233 (trainer:754) INFO: 189epoch:train:81-90batch: iter_time=2.055e-04, forward_time=0.054, loss=1.196, backward_time=0.047, grad_norm=0.212, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:25:37,925 (trainer:754) INFO: 189epoch:train:91-100batch: iter_time=1.251e-04, forward_time=0.051, loss=1.731, backward_time=0.047, grad_norm=0.178, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.338
[seoultech:0/4] 2025-01-24 02:25:39,671 (trainer:754) INFO: 189epoch:train:101-110batch: iter_time=1.377e-04, forward_time=0.057, loss=1.237, backward_time=0.047, grad_norm=0.206, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:26:19,492 (trainer:353) INFO: 189epoch results: [train] iter_time=0.003, forward_time=0.055, loss=1.480, backward_time=0.048, grad_norm=0.236, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.409, time=24.04 seconds, total_count=625042, gpu_max_cached_mem_GB=9.006, [valid] loss=1.437, time=5.07 seconds, total_count=1190, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.36 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:26:22,829 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:26:22,853 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/179epoch.pth
[seoultech:0/4] 2025-01-24 02:26:22,853 (trainer:287) INFO: 190/200epoch started. Estimated time to finish: 12 minutes and 19.08 seconds
[seoultech:0/4] 2025-01-24 02:26:28,435 (trainer:754) INFO: 190epoch:train:1-10batch: iter_time=0.037, forward_time=0.059, loss=1.479, backward_time=0.046, grad_norm=0.235, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.115
[seoultech:0/4] 2025-01-24 02:26:30,154 (trainer:754) INFO: 190epoch:train:11-20batch: iter_time=1.119e-04, forward_time=0.052, loss=1.489, backward_time=0.048, grad_norm=0.215, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:26:31,866 (trainer:754) INFO: 190epoch:train:21-30batch: iter_time=1.456e-04, forward_time=0.053, loss=1.268, backward_time=0.048, grad_norm=0.187, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:26:33,583 (trainer:754) INFO: 190epoch:train:31-40batch: iter_time=2.030e-04, forward_time=0.053, loss=1.624, backward_time=0.048, grad_norm=0.176, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:26:35,356 (trainer:754) INFO: 190epoch:train:41-50batch: iter_time=1.761e-04, forward_time=0.057, loss=1.589, backward_time=0.048, grad_norm=0.263, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 02:26:37,049 (trainer:754) INFO: 190epoch:train:51-60batch: iter_time=1.344e-04, forward_time=0.050, loss=1.727, backward_time=0.048, grad_norm=0.224, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.338
[seoultech:0/4] 2025-01-24 02:26:38,893 (trainer:754) INFO: 190epoch:train:61-70batch: iter_time=0.004, forward_time=0.060, loss=1.282, backward_time=0.046, grad_norm=0.231, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.368
[seoultech:0/4] 2025-01-24 02:26:40,613 (trainer:754) INFO: 190epoch:train:71-80batch: iter_time=1.280e-04, forward_time=0.054, loss=1.573, backward_time=0.047, grad_norm=0.222, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:26:42,294 (trainer:754) INFO: 190epoch:train:81-90batch: iter_time=1.761e-04, forward_time=0.051, loss=1.372, backward_time=0.046, grad_norm=0.224, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.336
[seoultech:0/4] 2025-01-24 02:26:43,976 (trainer:754) INFO: 190epoch:train:91-100batch: iter_time=1.452e-04, forward_time=0.051, loss=1.440, backward_time=0.047, grad_norm=0.222, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.336
[seoultech:0/4] 2025-01-24 02:26:45,728 (trainer:754) INFO: 190epoch:train:101-110batch: iter_time=1.619e-04, forward_time=0.056, loss=1.557, backward_time=0.047, grad_norm=0.218, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.350
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:27:26,536 (trainer:353) INFO: 190epoch results: [train] iter_time=0.004, forward_time=0.054, loss=1.472, backward_time=0.047, grad_norm=0.218, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.413, time=24.29 seconds, total_count=625156, gpu_max_cached_mem_GB=9.006, [valid] loss=1.432, time=5.77 seconds, total_count=1197, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.63 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:27:29,823 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:27:29,865 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/180epoch.pth
[seoultech:0/4] 2025-01-24 02:27:29,865 (trainer:287) INFO: 191/200epoch started. Estimated time to finish: 11 minutes and 11.84 seconds
[seoultech:0/4] 2025-01-24 02:27:35,285 (trainer:754) INFO: 191epoch:train:1-10batch: iter_time=0.041, forward_time=0.057, loss=1.371, backward_time=0.046, grad_norm=0.216, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.083
[seoultech:0/4] 2025-01-24 02:27:37,009 (trainer:754) INFO: 191epoch:train:11-20batch: iter_time=1.240e-04, forward_time=0.052, loss=1.220, backward_time=0.048, grad_norm=0.178, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:27:38,769 (trainer:754) INFO: 191epoch:train:21-30batch: iter_time=1.192e-04, forward_time=0.053, loss=1.354, backward_time=0.050, grad_norm=0.174, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:27:40,650 (trainer:754) INFO: 191epoch:train:31-40batch: iter_time=0.010, forward_time=0.058, loss=1.458, backward_time=0.047, grad_norm=0.205, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.376
[seoultech:0/4] 2025-01-24 02:27:42,474 (trainer:754) INFO: 191epoch:train:41-50batch: iter_time=0.002, forward_time=0.058, loss=1.447, backward_time=0.047, grad_norm=0.190, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.364
[seoultech:0/4] 2025-01-24 02:27:44,176 (trainer:754) INFO: 191epoch:train:51-60batch: iter_time=1.351e-04, forward_time=0.051, loss=1.538, backward_time=0.048, grad_norm=0.188, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:27:45,865 (trainer:754) INFO: 191epoch:train:61-70batch: iter_time=1.370e-04, forward_time=0.053, loss=1.331, backward_time=0.046, grad_norm=0.173, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.337
[seoultech:0/4] 2025-01-24 02:27:47,557 (trainer:754) INFO: 191epoch:train:71-80batch: iter_time=1.279e-04, forward_time=0.051, loss=1.753, backward_time=0.047, grad_norm=0.226, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.338
[seoultech:0/4] 2025-01-24 02:27:49,406 (trainer:754) INFO: 191epoch:train:81-90batch: iter_time=5.490e-04, forward_time=0.059, loss=1.413, backward_time=0.048, grad_norm=0.205, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.369
[seoultech:0/4] 2025-01-24 02:27:51,149 (trainer:754) INFO: 191epoch:train:91-100batch: iter_time=1.379e-04, forward_time=0.055, loss=1.540, backward_time=0.047, grad_norm=0.212, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:27:52,897 (trainer:754) INFO: 191epoch:train:101-110batch: iter_time=1.388e-04, forward_time=0.052, loss=1.479, backward_time=0.050, grad_norm=0.226, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:28:34,062 (trainer:353) INFO: 191epoch results: [train] iter_time=0.005, forward_time=0.054, loss=1.463, backward_time=0.047, grad_norm=0.202, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.416, time=24.63 seconds, total_count=625270, gpu_max_cached_mem_GB=9.006, [valid] loss=1.426, time=5.29 seconds, total_count=1204, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.27 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:28:37,201 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:28:37,225 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/181epoch.pth
[seoultech:0/4] 2025-01-24 02:28:37,225 (trainer:287) INFO: 192/200epoch started. Estimated time to finish: 10 minutes and 4.7 seconds
[seoultech:0/4] 2025-01-24 02:28:42,279 (trainer:754) INFO: 192epoch:train:1-10batch: iter_time=0.022, forward_time=0.054, loss=1.533, backward_time=0.050, grad_norm=0.185, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=1.010
[seoultech:0/4] 2025-01-24 02:28:43,988 (trainer:754) INFO: 192epoch:train:11-20batch: iter_time=1.836e-04, forward_time=0.052, loss=1.666, backward_time=0.048, grad_norm=0.182, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:28:45,702 (trainer:754) INFO: 192epoch:train:21-30batch: iter_time=1.247e-04, forward_time=0.050, loss=1.472, backward_time=0.047, grad_norm=0.184, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:28:47,388 (trainer:754) INFO: 192epoch:train:31-40batch: iter_time=1.568e-04, forward_time=0.048, loss=1.891, backward_time=0.048, grad_norm=0.174, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.337
[seoultech:0/4] 2025-01-24 02:28:49,119 (trainer:754) INFO: 192epoch:train:41-50batch: iter_time=1.255e-04, forward_time=0.051, loss=1.084, backward_time=0.047, grad_norm=0.231, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:28:50,857 (trainer:754) INFO: 192epoch:train:51-60batch: iter_time=1.217e-04, forward_time=0.054, loss=1.245, backward_time=0.046, grad_norm=0.188, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:28:52,620 (trainer:754) INFO: 192epoch:train:61-70batch: iter_time=1.299e-04, forward_time=0.057, loss=1.089, backward_time=0.047, grad_norm=0.190, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:28:54,477 (trainer:754) INFO: 192epoch:train:71-80batch: iter_time=0.005, forward_time=0.061, loss=1.542, backward_time=0.047, grad_norm=0.244, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.371
[seoultech:0/4] 2025-01-24 02:28:56,200 (trainer:754) INFO: 192epoch:train:81-90batch: iter_time=1.334e-04, forward_time=0.055, loss=1.519, backward_time=0.046, grad_norm=0.220, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:28:57,972 (trainer:754) INFO: 192epoch:train:91-100batch: iter_time=1.405e-04, forward_time=0.059, loss=1.334, backward_time=0.046, grad_norm=0.283, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 02:28:59,663 (trainer:754) INFO: 192epoch:train:101-110batch: iter_time=1.479e-04, forward_time=0.051, loss=1.561, backward_time=0.047, grad_norm=0.259, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.338
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:29:40,848 (trainer:353) INFO: 192epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.456, backward_time=0.047, grad_norm=0.215, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.406, time=24.06 seconds, total_count=625384, gpu_max_cached_mem_GB=9.006, [valid] loss=1.420, time=5.52 seconds, total_count=1211, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.04 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:29:44,183 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:29:44,207 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/182epoch.pth
[seoultech:0/4] 2025-01-24 02:29:44,207 (trainer:287) INFO: 193/200epoch started. Estimated time to finish: 8 minutes and 57.47 seconds
[seoultech:0/4] 2025-01-24 02:29:49,138 (trainer:754) INFO: 193epoch:train:1-10batch: iter_time=0.017, forward_time=0.057, loss=1.438, backward_time=0.047, grad_norm=0.303, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.985
[seoultech:0/4] 2025-01-24 02:29:50,852 (trainer:754) INFO: 193epoch:train:11-20batch: iter_time=1.193e-04, forward_time=0.051, loss=1.373, backward_time=0.047, grad_norm=0.241, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.342
[seoultech:0/4] 2025-01-24 02:29:52,575 (trainer:754) INFO: 193epoch:train:21-30batch: iter_time=1.311e-04, forward_time=0.054, loss=1.380, backward_time=0.047, grad_norm=0.240, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:29:54,274 (trainer:754) INFO: 193epoch:train:31-40batch: iter_time=1.498e-04, forward_time=0.051, loss=1.418, backward_time=0.047, grad_norm=0.220, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:29:56,000 (trainer:754) INFO: 193epoch:train:41-50batch: iter_time=1.340e-04, forward_time=0.052, loss=1.357, backward_time=0.047, grad_norm=0.244, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:29:57,759 (trainer:754) INFO: 193epoch:train:51-60batch: iter_time=1.213e-04, forward_time=0.053, loss=1.217, backward_time=0.047, grad_norm=0.226, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:29:59,465 (trainer:754) INFO: 193epoch:train:61-70batch: iter_time=1.438e-04, forward_time=0.051, loss=1.378, backward_time=0.047, grad_norm=0.230, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:30:01,201 (trainer:754) INFO: 193epoch:train:71-80batch: iter_time=1.813e-04, forward_time=0.053, loss=1.771, backward_time=0.048, grad_norm=0.239, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:30:02,907 (trainer:754) INFO: 193epoch:train:81-90batch: iter_time=1.295e-04, forward_time=0.050, loss=1.437, backward_time=0.048, grad_norm=0.197, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:30:04,681 (trainer:754) INFO: 193epoch:train:91-100batch: iter_time=1.167e-04, forward_time=0.056, loss=1.765, backward_time=0.048, grad_norm=0.197, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:30:06,437 (trainer:754) INFO: 193epoch:train:101-110batch: iter_time=1.253e-04, forward_time=0.054, loss=1.403, backward_time=0.048, grad_norm=0.202, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.351
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:30:48,505 (trainer:353) INFO: 193epoch results: [train] iter_time=0.002, forward_time=0.054, loss=1.449, backward_time=0.047, grad_norm=0.234, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.407, time=23.98 seconds, total_count=625498, gpu_max_cached_mem_GB=9.006, [valid] loss=1.418, time=5.9 seconds, total_count=1218, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.41 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:30:51,701 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:30:51,725 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/183epoch.pth
[seoultech:0/4] 2025-01-24 02:30:51,725 (trainer:287) INFO: 194/200epoch started. Estimated time to finish: 7 minutes and 50.34 seconds
[seoultech:0/4] 2025-01-24 02:30:57,139 (trainer:754) INFO: 194epoch:train:1-10batch: iter_time=0.024, forward_time=0.056, loss=1.425, backward_time=0.052, grad_norm=0.172, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.015, optim0_lr0=0.001, train_time=1.081
[seoultech:0/4] 2025-01-24 02:30:58,917 (trainer:754) INFO: 194epoch:train:11-20batch: iter_time=1.983e-04, forward_time=0.057, loss=1.361, backward_time=0.047, grad_norm=0.201, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.356
[seoultech:0/4] 2025-01-24 02:31:00,917 (trainer:754) INFO: 194epoch:train:21-30batch: iter_time=0.003, forward_time=0.056, loss=1.761, backward_time=0.047, grad_norm=0.237, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.400
[seoultech:0/4] 2025-01-24 02:31:02,730 (trainer:754) INFO: 194epoch:train:31-40batch: iter_time=0.008, forward_time=0.056, loss=1.468, backward_time=0.047, grad_norm=0.219, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.011, optim0_lr0=0.001, train_time=0.363
[seoultech:0/4] 2025-01-24 02:31:04,474 (trainer:754) INFO: 194epoch:train:41-50batch: iter_time=1.544e-04, forward_time=0.053, loss=1.306, backward_time=0.047, grad_norm=0.246, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:31:06,199 (trainer:754) INFO: 194epoch:train:51-60batch: iter_time=1.856e-04, forward_time=0.051, loss=1.351, backward_time=0.049, grad_norm=0.157, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:31:08,010 (trainer:754) INFO: 194epoch:train:61-70batch: iter_time=1.437e-04, forward_time=0.055, loss=1.539, backward_time=0.048, grad_norm=0.203, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.362
[seoultech:0/4] 2025-01-24 02:31:09,729 (trainer:754) INFO: 194epoch:train:71-80batch: iter_time=1.922e-04, forward_time=0.051, loss=1.643, backward_time=0.049, grad_norm=0.166, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:31:11,475 (trainer:754) INFO: 194epoch:train:81-90batch: iter_time=1.689e-04, forward_time=0.054, loss=1.110, backward_time=0.048, grad_norm=0.168, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:31:13,343 (trainer:754) INFO: 194epoch:train:91-100batch: iter_time=1.506e-04, forward_time=0.061, loss=1.578, backward_time=0.047, grad_norm=0.201, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.373
[seoultech:0/4] 2025-01-24 02:31:15,060 (trainer:754) INFO: 194epoch:train:101-110batch: iter_time=1.772e-04, forward_time=0.052, loss=1.451, backward_time=0.046, grad_norm=0.184, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:31:55,533 (trainer:353) INFO: 194epoch results: [train] iter_time=0.003, forward_time=0.055, loss=1.441, backward_time=0.048, grad_norm=0.196, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.422, time=24.78 seconds, total_count=625612, gpu_max_cached_mem_GB=9.006, [valid] loss=1.422, time=5.49 seconds, total_count=1225, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.54 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:31:58,859 (trainer:406) INFO: There are no improvements in this epoch
[seoultech:0/4] 2025-01-24 02:31:58,884 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/184epoch.pth
[seoultech:0/4] 2025-01-24 02:31:58,884 (trainer:287) INFO: 195/200epoch started. Estimated time to finish: 6 minutes and 43.14 seconds
[seoultech:0/4] 2025-01-24 02:32:04,298 (trainer:754) INFO: 195epoch:train:1-10batch: iter_time=0.024, forward_time=0.067, loss=1.504, backward_time=0.048, grad_norm=0.192, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.016, optim0_lr0=0.001, train_time=1.081
[seoultech:0/4] 2025-01-24 02:32:06,075 (trainer:754) INFO: 195epoch:train:11-20batch: iter_time=2.217e-04, forward_time=0.059, loss=1.353, backward_time=0.047, grad_norm=0.203, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:32:07,814 (trainer:754) INFO: 195epoch:train:21-30batch: iter_time=1.481e-04, forward_time=0.052, loss=1.322, backward_time=0.048, grad_norm=0.215, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:32:09,573 (trainer:754) INFO: 195epoch:train:31-40batch: iter_time=1.408e-04, forward_time=0.059, loss=1.242, backward_time=0.047, grad_norm=0.199, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:32:11,304 (trainer:754) INFO: 195epoch:train:41-50batch: iter_time=1.382e-04, forward_time=0.054, loss=1.574, backward_time=0.048, grad_norm=0.280, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:32:13,046 (trainer:754) INFO: 195epoch:train:51-60batch: iter_time=1.428e-04, forward_time=0.057, loss=1.421, backward_time=0.046, grad_norm=0.187, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:32:14,754 (trainer:754) INFO: 195epoch:train:61-70batch: iter_time=1.421e-04, forward_time=0.050, loss=1.675, backward_time=0.048, grad_norm=0.185, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:32:16,536 (trainer:754) INFO: 195epoch:train:71-80batch: iter_time=1.368e-04, forward_time=0.058, loss=1.482, backward_time=0.048, grad_norm=0.208, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.356
[seoultech:0/4] 2025-01-24 02:32:18,239 (trainer:754) INFO: 195epoch:train:81-90batch: iter_time=1.288e-04, forward_time=0.052, loss=1.448, backward_time=0.047, grad_norm=0.183, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:32:19,982 (trainer:754) INFO: 195epoch:train:91-100batch: iter_time=1.869e-04, forward_time=0.054, loss=1.319, backward_time=0.048, grad_norm=0.183, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:32:21,730 (trainer:754) INFO: 195epoch:train:101-110batch: iter_time=1.812e-04, forward_time=0.055, loss=1.469, backward_time=0.047, grad_norm=0.198, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:33:02,268 (trainer:353) INFO: 195epoch results: [train] iter_time=0.002, forward_time=0.056, loss=1.433, backward_time=0.047, grad_norm=0.203, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.413, time=24.29 seconds, total_count=625726, gpu_max_cached_mem_GB=9.006, [valid] loss=1.418, time=5.47 seconds, total_count=1232, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.63 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:33:05,821 (trainer:406) INFO: There are no improvements in this epoch
[seoultech:0/4] 2025-01-24 02:33:05,846 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/186epoch.pth
[seoultech:0/4] 2025-01-24 02:33:05,846 (trainer:287) INFO: 196/200epoch started. Estimated time to finish: 5 minutes and 35.93 seconds
[seoultech:0/4] 2025-01-24 02:33:10,872 (trainer:754) INFO: 196epoch:train:1-10batch: iter_time=0.020, forward_time=0.055, loss=1.219, backward_time=0.047, grad_norm=0.173, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.004
[seoultech:0/4] 2025-01-24 02:33:12,902 (trainer:754) INFO: 196epoch:train:11-20batch: iter_time=2.733e-04, forward_time=0.059, loss=1.588, backward_time=0.049, grad_norm=0.227, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.406
[seoultech:0/4] 2025-01-24 02:33:14,708 (trainer:754) INFO: 196epoch:train:21-30batch: iter_time=0.001, forward_time=0.057, loss=1.583, backward_time=0.047, grad_norm=0.194, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.361
[seoultech:0/4] 2025-01-24 02:33:16,456 (trainer:754) INFO: 196epoch:train:31-40batch: iter_time=1.663e-04, forward_time=0.054, loss=1.354, backward_time=0.048, grad_norm=0.165, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:33:18,111 (trainer:754) INFO: 196epoch:train:41-50batch: iter_time=1.805e-04, forward_time=0.050, loss=1.346, backward_time=0.046, grad_norm=0.167, clip=0.000e+00, loss_scale=8.113e+31, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.331
[seoultech:0/4] 2025-01-24 02:33:19,863 (trainer:754) INFO: 196epoch:train:51-60batch: iter_time=1.874e-04, forward_time=0.056, loss=1.345, backward_time=0.047, grad_norm=0.178, clip=0.000e+00, loss_scale=1.298e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:33:21,589 (trainer:754) INFO: 196epoch:train:61-70batch: iter_time=1.765e-04, forward_time=0.053, loss=1.487, backward_time=0.049, grad_norm=0.200, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:33:23,318 (trainer:754) INFO: 196epoch:train:71-80batch: iter_time=1.494e-04, forward_time=0.053, loss=1.347, backward_time=0.048, grad_norm=0.170, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:33:25,052 (trainer:754) INFO: 196epoch:train:81-90batch: iter_time=1.961e-04, forward_time=0.055, loss=1.339, backward_time=0.047, grad_norm=0.185, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:33:26,753 (trainer:754) INFO: 196epoch:train:91-100batch: iter_time=1.601e-04, forward_time=0.051, loss=1.398, backward_time=0.048, grad_norm=0.180, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:33:28,482 (trainer:754) INFO: 196epoch:train:101-110batch: iter_time=2.118e-04, forward_time=0.054, loss=1.635, backward_time=0.047, grad_norm=0.184, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:34:09,535 (trainer:353) INFO: 196epoch results: [train] iter_time=0.002, forward_time=0.054, loss=1.424, backward_time=0.048, grad_norm=0.184, clip=0.000e+00, loss_scale=1.238e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.409, time=24.14 seconds, total_count=625840, gpu_max_cached_mem_GB=9.006, [valid] loss=1.410, time=5.56 seconds, total_count=1239, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.99 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:34:13,096 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:34:13,122 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/185epoch.pth
[seoultech:0/4] 2025-01-24 02:34:13,122 (trainer:287) INFO: 197/200epoch started. Estimated time to finish: 4 minutes and 28.75 seconds
[seoultech:0/4] 2025-01-24 02:34:18,109 (trainer:754) INFO: 197epoch:train:1-10batch: iter_time=0.017, forward_time=0.057, loss=1.300, backward_time=0.053, grad_norm=0.172, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.996
[seoultech:0/4] 2025-01-24 02:34:19,869 (trainer:754) INFO: 197epoch:train:11-20batch: iter_time=2.126e-04, forward_time=0.053, loss=1.622, backward_time=0.049, grad_norm=0.180, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:34:21,583 (trainer:754) INFO: 197epoch:train:21-30batch: iter_time=1.647e-04, forward_time=0.052, loss=1.426, backward_time=0.047, grad_norm=0.157, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:34:23,571 (trainer:754) INFO: 197epoch:train:31-40batch: iter_time=0.009, forward_time=0.066, loss=1.264, backward_time=0.047, grad_norm=0.219, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.397
[seoultech:0/4] 2025-01-24 02:34:25,249 (trainer:754) INFO: 197epoch:train:41-50batch: iter_time=2.244e-04, forward_time=0.051, loss=1.369, backward_time=0.048, grad_norm=0.205, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.014, optim0_lr0=0.001, train_time=0.335
[seoultech:0/4] 2025-01-24 02:34:26,988 (trainer:754) INFO: 197epoch:train:51-60batch: iter_time=1.765e-04, forward_time=0.055, loss=1.415, backward_time=0.048, grad_norm=0.243, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:34:28,696 (trainer:754) INFO: 197epoch:train:61-70batch: iter_time=1.410e-04, forward_time=0.052, loss=1.147, backward_time=0.048, grad_norm=0.224, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.341
[seoultech:0/4] 2025-01-24 02:34:30,444 (trainer:754) INFO: 197epoch:train:71-80batch: iter_time=1.372e-04, forward_time=0.054, loss=1.578, backward_time=0.048, grad_norm=0.228, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:34:32,178 (trainer:754) INFO: 197epoch:train:81-90batch: iter_time=1.494e-04, forward_time=0.052, loss=1.464, backward_time=0.048, grad_norm=0.227, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:34:33,910 (trainer:754) INFO: 197epoch:train:91-100batch: iter_time=1.273e-04, forward_time=0.052, loss=1.621, backward_time=0.048, grad_norm=0.252, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:34:35,680 (trainer:754) INFO: 197epoch:train:101-110batch: iter_time=1.516e-04, forward_time=0.060, loss=1.474, backward_time=0.046, grad_norm=0.253, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.354
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:35:16,689 (trainer:353) INFO: 197epoch results: [train] iter_time=0.002, forward_time=0.055, loss=1.420, backward_time=0.048, grad_norm=0.217, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.407, time=24 seconds, total_count=625954, gpu_max_cached_mem_GB=9.006, [valid] loss=1.411, time=5.75 seconds, total_count=1246, gpu_max_cached_mem_GB=9.006, [att_plot] time=33.81 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:35:20,215 (trainer:406) INFO: There are no improvements in this epoch
[seoultech:0/4] 2025-01-24 02:35:20,240 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/187epoch.pth
[seoultech:0/4] 2025-01-24 02:35:20,240 (trainer:287) INFO: 198/200epoch started. Estimated time to finish: 3 minutes and 21.56 seconds
[seoultech:0/4] 2025-01-24 02:35:25,343 (trainer:754) INFO: 198epoch:train:1-10batch: iter_time=0.029, forward_time=0.057, loss=1.351, backward_time=0.047, grad_norm=0.238, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=1.019
[seoultech:0/4] 2025-01-24 02:35:27,083 (trainer:754) INFO: 198epoch:train:11-20batch: iter_time=1.741e-04, forward_time=0.057, loss=1.367, backward_time=0.046, grad_norm=0.237, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:35:28,831 (trainer:754) INFO: 198epoch:train:21-30batch: iter_time=1.532e-04, forward_time=0.054, loss=1.595, backward_time=0.048, grad_norm=0.253, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.349
[seoultech:0/4] 2025-01-24 02:35:30,616 (trainer:754) INFO: 198epoch:train:31-40batch: iter_time=2.014e-04, forward_time=0.056, loss=1.506, backward_time=0.049, grad_norm=0.244, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.357
[seoultech:0/4] 2025-01-24 02:35:32,346 (trainer:754) INFO: 198epoch:train:41-50batch: iter_time=1.711e-04, forward_time=0.054, loss=1.332, backward_time=0.047, grad_norm=0.216, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.346
[seoultech:0/4] 2025-01-24 02:35:34,068 (trainer:754) INFO: 198epoch:train:51-60batch: iter_time=1.976e-04, forward_time=0.053, loss=1.337, backward_time=0.047, grad_norm=0.239, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:35:35,811 (trainer:754) INFO: 198epoch:train:61-70batch: iter_time=1.482e-04, forward_time=0.055, loss=1.548, backward_time=0.047, grad_norm=0.254, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.348
[seoultech:0/4] 2025-01-24 02:35:37,528 (trainer:754) INFO: 198epoch:train:71-80batch: iter_time=1.971e-04, forward_time=0.052, loss=1.415, backward_time=0.048, grad_norm=0.172, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:35:39,262 (trainer:754) INFO: 198epoch:train:81-90batch: iter_time=1.759e-04, forward_time=0.054, loss=1.257, backward_time=0.048, grad_norm=0.164, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:35:40,959 (trainer:754) INFO: 198epoch:train:91-100batch: iter_time=1.402e-04, forward_time=0.051, loss=1.506, backward_time=0.047, grad_norm=0.175, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.339
[seoultech:0/4] 2025-01-24 02:35:42,709 (trainer:754) INFO: 198epoch:train:101-110batch: iter_time=2.071e-04, forward_time=0.056, loss=1.343, backward_time=0.047, grad_norm=0.230, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:36:23,930 (trainer:353) INFO: 198epoch results: [train] iter_time=0.003, forward_time=0.054, loss=1.414, backward_time=0.047, grad_norm=0.219, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.406, time=23.87 seconds, total_count=626068, gpu_max_cached_mem_GB=9.006, [valid] loss=1.394, time=5.38 seconds, total_count=1253, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.45 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:36:27,280 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:36:27,305 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/188epoch.pth
[seoultech:0/4] 2025-01-24 02:36:27,305 (trainer:287) INFO: 199/200epoch started. Estimated time to finish: 2 minutes and 14.37 seconds
[seoultech:0/4] 2025-01-24 02:36:32,683 (trainer:754) INFO: 199epoch:train:1-10batch: iter_time=0.033, forward_time=0.064, loss=1.299, backward_time=0.047, grad_norm=0.188, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.014, optim0_lr0=0.001, train_time=1.074
[seoultech:0/4] 2025-01-24 02:36:34,536 (trainer:754) INFO: 199epoch:train:11-20batch: iter_time=2.626e-04, forward_time=0.056, loss=1.396, backward_time=0.047, grad_norm=0.177, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.370
[seoultech:0/4] 2025-01-24 02:36:36,253 (trainer:754) INFO: 199epoch:train:21-30batch: iter_time=1.387e-04, forward_time=0.053, loss=1.243, backward_time=0.047, grad_norm=0.170, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:36:37,969 (trainer:754) INFO: 199epoch:train:31-40batch: iter_time=1.972e-04, forward_time=0.053, loss=1.281, backward_time=0.048, grad_norm=0.257, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.343
[seoultech:0/4] 2025-01-24 02:36:39,726 (trainer:754) INFO: 199epoch:train:41-50batch: iter_time=2.078e-04, forward_time=0.056, loss=1.258, backward_time=0.047, grad_norm=0.191, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.351
[seoultech:0/4] 2025-01-24 02:36:41,487 (trainer:754) INFO: 199epoch:train:51-60batch: iter_time=2.306e-04, forward_time=0.054, loss=1.647, backward_time=0.049, grad_norm=0.210, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.352
[seoultech:0/4] 2025-01-24 02:36:43,213 (trainer:754) INFO: 199epoch:train:61-70batch: iter_time=2.323e-04, forward_time=0.053, loss=1.131, backward_time=0.047, grad_norm=0.176, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:36:44,949 (trainer:754) INFO: 199epoch:train:71-80batch: iter_time=1.573e-04, forward_time=0.054, loss=1.325, backward_time=0.047, grad_norm=0.176, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.347
[seoultech:0/4] 2025-01-24 02:36:46,725 (trainer:754) INFO: 199epoch:train:81-90batch: iter_time=1.540e-04, forward_time=0.056, loss=1.571, backward_time=0.048, grad_norm=0.176, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.355
[seoultech:0/4] 2025-01-24 02:36:48,643 (trainer:754) INFO: 199epoch:train:91-100batch: iter_time=0.016, forward_time=0.057, loss=1.560, backward_time=0.048, grad_norm=0.212, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.383
[seoultech:0/4] 2025-01-24 02:36:50,357 (trainer:754) INFO: 199epoch:train:101-110batch: iter_time=1.647e-04, forward_time=0.052, loss=1.720, backward_time=0.047, grad_norm=0.182, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.343
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:37:31,736 (trainer:353) INFO: 199epoch results: [train] iter_time=0.004, forward_time=0.055, loss=1.405, backward_time=0.047, grad_norm=0.192, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.416, time=24.48 seconds, total_count=626182, gpu_max_cached_mem_GB=9.006, [valid] loss=1.395, time=5.78 seconds, total_count=1260, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.18 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:37:35,071 (trainer:406) INFO: There are no improvements in this epoch
[seoultech:0/4] 2025-01-24 02:37:35,096 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/189epoch.pth
[seoultech:0/4] 2025-01-24 02:37:35,096 (trainer:287) INFO: 200/200epoch started. Estimated time to finish: 1 minute and 7.2 seconds
[seoultech:0/4] 2025-01-24 02:37:40,060 (trainer:754) INFO: 200epoch:train:1-10batch: iter_time=0.019, forward_time=0.052, loss=1.413, backward_time=0.049, grad_norm=0.181, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.992
[seoultech:0/4] 2025-01-24 02:37:41,834 (trainer:754) INFO: 200epoch:train:11-20batch: iter_time=1.359e-04, forward_time=0.059, loss=1.414, backward_time=0.046, grad_norm=0.179, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.354
[seoultech:0/4] 2025-01-24 02:37:43,524 (trainer:754) INFO: 200epoch:train:21-30batch: iter_time=1.267e-04, forward_time=0.050, loss=1.398, backward_time=0.048, grad_norm=0.158, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.338
[seoultech:0/4] 2025-01-24 02:37:45,391 (trainer:754) INFO: 200epoch:train:31-40batch: iter_time=0.008, forward_time=0.057, loss=1.478, backward_time=0.047, grad_norm=0.198, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.373
[seoultech:0/4] 2025-01-24 02:37:47,091 (trainer:754) INFO: 200epoch:train:41-50batch: iter_time=1.310e-04, forward_time=0.049, loss=1.463, backward_time=0.047, grad_norm=0.177, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.340
[seoultech:0/4] 2025-01-24 02:37:48,810 (trainer:754) INFO: 200epoch:train:51-60batch: iter_time=0.001, forward_time=0.052, loss=1.506, backward_time=0.047, grad_norm=0.186, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.344
[seoultech:0/4] 2025-01-24 02:37:50,537 (trainer:754) INFO: 200epoch:train:61-70batch: iter_time=3.537e-04, forward_time=0.055, loss=1.321, backward_time=0.046, grad_norm=0.165, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:37:52,225 (trainer:754) INFO: 200epoch:train:71-80batch: iter_time=1.388e-04, forward_time=0.051, loss=1.255, backward_time=0.048, grad_norm=0.201, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.337
[seoultech:0/4] 2025-01-24 02:37:53,953 (trainer:754) INFO: 200epoch:train:81-90batch: iter_time=1.563e-04, forward_time=0.054, loss=1.296, backward_time=0.048, grad_norm=0.204, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.345
[seoultech:0/4] 2025-01-24 02:37:55,702 (trainer:754) INFO: 200epoch:train:91-100batch: iter_time=1.753e-04, forward_time=0.054, loss=1.627, backward_time=0.050, grad_norm=0.204, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.013, optim0_lr0=0.001, train_time=0.350
[seoultech:0/4] 2025-01-24 02:37:57,482 (trainer:754) INFO: 200epoch:train:101-110batch: iter_time=1.617e-04, forward_time=0.058, loss=1.221, backward_time=0.047, grad_norm=0.203, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.356
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
/data/bootcamp2501/espnet/tools/miniconda_py38/envs/espnet/lib/python3.8/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: [1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.[0m
  def backtrace(trace: np.ndarray):
[seoultech:0/4] 2025-01-24 02:38:39,241 (trainer:353) INFO: 200epoch results: [train] iter_time=0.003, forward_time=0.053, loss=1.397, backward_time=0.047, grad_norm=0.187, clip=0.000e+00, loss_scale=1.623e+32, optim_step_time=0.012, optim0_lr0=0.001, train_time=0.404, time=23.76 seconds, total_count=626296, gpu_max_cached_mem_GB=9.006, [valid] loss=1.391, time=5.62 seconds, total_count=1267, gpu_max_cached_mem_GB=9.006, [att_plot] time=34.77 seconds, total_count=0, gpu_max_cached_mem_GB=9.006
[seoultech:0/4] 2025-01-24 02:38:42,602 (trainer:408) INFO: The best model has been updated: valid.loss
[seoultech:0/4] 2025-01-24 02:38:42,630 (trainer:462) INFO: The model files were removed: exp_ex3/lm_train_lm_transformer2_en_bpe5000/190epoch.pth
[seoultech:0/4] 2025-01-24 02:38:42,630 (trainer:480) INFO: The training was finished at 200 epochs 
[seoultech:0/4] 2025-01-24 02:38:42,666 (average_nbest_models:69) INFO: Averaging 10best models: criterion="valid.loss": exp_ex3/lm_train_lm_transformer2_en_bpe5000/valid.loss.ave_10best.pth
# Accounting: time=3375 threads=1
# Ended (code 0) at Fri Jan 24 02:38:46 KST 2025, elapsed time 3375 seconds
